{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import pandas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import tree\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "import shap\n",
    "import lightgbm as lgb \n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from optuna.integration import LightGBMPruningCallback\n",
    "from optuna.pruners import MedianPruner\n",
    "import xgboost\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow import keras\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from tensorflow.keras.metrics import Accuracy, BinaryCrossentropy\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, chi2, f_classif, mutual_info_regression, mutual_info_classif, VarianceThreshold\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training data set: (162500, 166)\n",
      "Shape of test data set: (160651, 164)\n",
      "Shape of X: (162500, 160)\n",
      "Shape of y: (162500,)\n",
      "41005\n",
      "121495\n"
     ]
    }
   ],
   "source": [
    "# read data\n",
    "def load_data(name):\n",
    "    with h5py.File(f'{name}.h5', 'r') as f:\n",
    "        filename = name.split('/')[-1]\n",
    "        return pandas.DataFrame(f[filename][:], dtype=np.float64)\n",
    "\n",
    "train = load_data('train')\n",
    "test  = load_data('test')\n",
    "\n",
    "print (f'Shape of training data set: {train.shape}')\n",
    "print (f'Shape of test data set: {test.shape}')\n",
    "\n",
    "all_variables = ['actualInteractionsPerCrossing', 'averageInteractionsPerCrossing', 'correctedActualMu', 'correctedAverageMu', 'correctedScaledActualMu', 'correctedScaledAverageMu', 'NvtxReco', 'p_nTracks', 'p_pt_track', 'p_eta', 'p_phi', 'p_charge', 'p_qOverP', 'p_z0', 'p_d0', 'p_sigmad0', 'p_d0Sig', 'p_EptRatio', 'p_dPOverP', 'p_z0theta', 'p_etaCluster', 'p_phiCluster', 'p_eCluster', 'p_rawEtaCluster', 'p_rawPhiCluster', 'p_rawECluster', 'p_eClusterLr0', 'p_eClusterLr1', 'p_eClusterLr2', 'p_eClusterLr3', 'p_etaClusterLr1', 'p_etaClusterLr2', 'p_phiClusterLr2', 'p_eAccCluster', 'p_f0Cluster', 'p_etaCalo', 'p_phiCalo', 'p_eTileGap3Cluster', 'p_cellIndexCluster', 'p_phiModCalo', 'p_etaModCalo', 'p_dPhiTH3', 'p_R12', 'p_fTG3', 'p_weta2', 'p_Reta', 'p_Rphi', 'p_Eratio', 'p_f1', 'p_f3', 'p_Rhad', 'p_Rhad1', 'p_deltaEta1', 'p_deltaPhiRescaled2', 'p_TRTPID', 'p_TRTTrackOccupancy', 'p_numberOfInnermostPixelHits', 'p_numberOfPixelHits', 'p_numberOfSCTHits', 'p_numberOfTRTHits', 'p_numberOfTRTXenonHits', 'p_chi2', 'p_ndof', 'p_SharedMuonTrack', 'p_E7x7_Lr2', 'p_E7x7_Lr3', 'p_E_Lr0_HiG', 'p_E_Lr0_LowG', 'p_E_Lr0_MedG', 'p_E_Lr1_HiG', 'p_E_Lr1_LowG', 'p_E_Lr1_MedG', 'p_E_Lr2_HiG', 'p_E_Lr2_LowG', 'p_E_Lr2_MedG', 'p_E_Lr3_HiG', 'p_E_Lr3_LowG', 'p_E_Lr3_MedG', 'p_ambiguityType', 'p_asy1', 'p_author', 'p_barys1', 'p_core57cellsEnergyCorrection', 'p_deltaEta0', 'p_deltaEta2', 'p_deltaEta3', 'p_deltaPhi0', 'p_deltaPhi1', 'p_deltaPhi2', 'p_deltaPhi3', 'p_deltaPhiFromLastMeasurement', 'p_deltaPhiRescaled0', 'p_deltaPhiRescaled1', 'p_deltaPhiRescaled3', 'p_e1152', 'p_e132', 'p_e235', 'p_e255', 'p_e2ts1', 'p_ecore', 'p_emins1', 'p_etconeCorrBitset', 'p_ethad', 'p_ethad1', 'p_f1core', 'p_f3core', 'p_maxEcell_energy', 'p_maxEcell_gain', 'p_maxEcell_time', 'p_maxEcell_x', 'p_maxEcell_y', 'p_maxEcell_z', 'p_nCells_Lr0_HiG', 'p_nCells_Lr0_LowG', 'p_nCells_Lr0_MedG', 'p_nCells_Lr1_HiG', 'p_nCells_Lr1_LowG', 'p_nCells_Lr1_MedG', 'p_nCells_Lr2_HiG', 'p_nCells_Lr2_LowG', 'p_nCells_Lr2_MedG', 'p_nCells_Lr3_HiG', 'p_nCells_Lr3_LowG', 'p_nCells_Lr3_MedG', 'p_pos', 'p_pos7', 'p_poscs1', 'p_poscs2', 'p_ptconeCorrBitset', 'p_ptconecoreTrackPtrCorrection', 'p_r33over37allcalo', 'p_topoetconeCorrBitset', 'p_topoetconecoreConeEnergyCorrection', 'p_topoetconecoreConeSCEnergyCorrection', 'p_weta1', 'p_widths1', 'p_widths2', 'p_wtots1', 'p_e233', 'p_e237', 'p_e277', 'p_e2tsts1', 'p_ehad1', 'p_emaxs1', 'p_fracs1', 'p_DeltaE', 'p_E3x5_Lr0', 'p_E3x5_Lr1', 'p_E3x5_Lr2', 'p_E3x5_Lr3', 'p_E5x7_Lr0', 'p_E5x7_Lr1', 'p_E5x7_Lr2', 'p_E5x7_Lr3', 'p_E7x11_Lr0', 'p_E7x11_Lr1', 'p_E7x11_Lr2', 'p_E7x11_Lr3', 'p_E7x7_Lr0', 'p_E7x7_Lr1' ]\n",
    "\n",
    "\n",
    "X = train[all_variables]\n",
    "sc_X = preprocessing.StandardScaler()\n",
    "X = sc_X.fit_transform(X)\n",
    "y = train['Truth']\n",
    "\n",
    "# X = pd.DataFrame(X, columns=all_variables)\n",
    "\n",
    "print (f'Shape of X: {X.shape}')\n",
    "print (f'Shape of y: {y.shape}')\n",
    "\n",
    "print(sum(y==0))\n",
    "print(sum(y==1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shap value xgbooster\n",
    "\n",
    "def shap_xgbooster():\n",
    "    model = xgboost.XGBRegressor().fit(X, y)\n",
    "    explainer = shap.Explainer(model)\n",
    "    shap_values = explainer(X)\n",
    "\n",
    "    feature_names = shap_values.feature_names\n",
    "    shap_df = pd.DataFrame(shap_values.values, columns=feature_names)\n",
    "    vals = np.abs(shap_df.values).mean(0)\n",
    "    shap_importance = pd.DataFrame(list(zip(feature_names, vals)), columns=['col_name', 'feature_importance_vals'])\n",
    "    shap_importance.sort_values(by=['feature_importance_vals'], ascending=False, inplace=True)\n",
    "    shap.plots.bar(shap_values)\n",
    "\n",
    "    return shap_importance.head(20)\n",
    "\n",
    "shape_variables = ['p_Rhad', 'p_Rphi', 'p_Reta', 'p_sigmad0', 'p_deltaEta1', 'p_ptconecoreTrackPtrCorrection', 'p_deltaPhiRescaled2', 'p_d0', 'p_numberOfInnermostPixelHits', 'p_ambiguityType',\n",
    "                    'p_rawPhiCluster','p_phiCalo', 'p_ethad', 'p_EptRatio', 'p_Rhad1', 'p_E7x11_Lr3', 'p_ehad1', 'p_Eratio', 'p_deltaPhi2', 'p_nTracks']\n",
    "\n",
    "shap_variables = ['p_Rhad','p_Reta','p_deltaEta1', 'p_sigmad0',\n",
    "\t'p_Rphi',\t\n",
    "\t'p_ambiguityType',\t\n",
    "\t'p_ethad',\n",
    "\t'p_numberOfInnermostPixelHits',\n",
    "\t'p_deltaPhiRescaled2',\n",
    "\t'p_ptconecoreTrackPtrCorrection',\n",
    "\t'p_d0',\n",
    "\t'p_Rhad1',\n",
    "\t'p_d0Sig',\n",
    "\t'p_nTracks',\n",
    "\t'p_deltaPhiFromLastMeasurement',\n",
    "\t'p_E7x11_Lr3',\n",
    "\t'p_deltaPhi2',\n",
    "\t'p_numberOfPixelHits',\t\n",
    "\t'p_EptRatio',\t\n",
    "\t'p_dPOverP']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cols</th>\n",
       "      <th>feature_imp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>p_sigmad0</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>p_deltaPhiRescaled2</td>\n",
       "      <td>130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>p_deltaEta1</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>p_Reta</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>p_d0</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>p_ptconecoreTrackPtrCorrection</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>p_d0Sig</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>p_Rhad</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>p_Rphi</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>p_deltaPhiFromLastMeasurement</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>p_deltaPhi2</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>p_EptRatio</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>p_dPOverP</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>p_numberOfSCTHits</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>p_numberOfPixelHits</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>p_pt_track</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>p_nTracks</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>p_ethad</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>p_qOverP</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>p_ambiguityType</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               cols  feature_imp\n",
       "15                        p_sigmad0          150\n",
       "53              p_deltaPhiRescaled2          130\n",
       "52                      p_deltaEta1          124\n",
       "45                           p_Reta          123\n",
       "14                             p_d0          123\n",
       "129  p_ptconecoreTrackPtrCorrection          120\n",
       "16                          p_d0Sig          106\n",
       "50                           p_Rhad           88\n",
       "46                           p_Rphi           85\n",
       "90    p_deltaPhiFromLastMeasurement           80\n",
       "88                      p_deltaPhi2           79\n",
       "17                       p_EptRatio           79\n",
       "18                        p_dPOverP           64\n",
       "58                p_numberOfSCTHits           63\n",
       "57              p_numberOfPixelHits           62\n",
       "8                        p_pt_track           61\n",
       "7                         p_nTracks           58\n",
       "102                         p_ethad           48\n",
       "12                         p_qOverP           48\n",
       "78                  p_ambiguityType           48"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# feature ranking lgb\n",
    "\n",
    "def feature_importance_lgb():\n",
    "    gbm = lgb.LGBMRegressor()\n",
    "    gbm.fit(X_train, y_train)\n",
    "    gbm.booster_.feature_importance()\n",
    "\n",
    "    feature_imp_ = pd.DataFrame({'cols':X_train.columns, 'feature_imp':gbm.feature_importances_})\n",
    "    feature_imp_.loc[feature_imp_.feature_imp > 0].sort_values(by=['feature_imp'], ascending=False)\n",
    "\n",
    "    return feature_imp_.loc[feature_imp_.feature_imp > 0].sort_values(by=['feature_imp'], ascending=False).head(20)\n",
    "\n",
    "feature_importance_lgb()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model shape:  (162500, 20)\n",
      "model scores: [0.00000000e+00 2.00523377e-03 6.00631961e-04 0.00000000e+00\n",
      " 0.00000000e+00 2.57362420e-04 1.21159116e-03 6.68945328e-02\n",
      " 8.66664990e-02 0.00000000e+00 4.63271514e-04 1.17064972e-02\n",
      " 7.45827949e-02 2.25841968e-04 6.46400995e-02 6.29030490e-02\n",
      " 4.78289453e-02 8.40571638e-02 3.06450910e-02 2.51914230e-05\n",
      " 4.29705808e-04 3.49969640e-04 1.78873260e-03 3.70970184e-04\n",
      " 3.79191462e-04 1.83444385e-03 1.33911324e-03 2.14129361e-02\n",
      " 2.52082943e-03 1.13266801e-01 1.27613345e-03 9.30126864e-03\n",
      " 3.75154596e-04 2.99390611e-03 2.35652111e-03 4.90189428e-04\n",
      " 1.18442724e-03 1.11187978e-03 1.41327540e-03 1.81302056e-03\n",
      " 7.10784781e-05 0.00000000e+00 8.86168347e-03 1.91565655e-03\n",
      " 1.45343261e-01 2.21468850e-01 1.70567877e-01 1.77671862e-01\n",
      " 1.08817552e-02 1.14928325e-01 2.44758413e-01 2.35569220e-01\n",
      " 2.31145431e-01 1.66139071e-01 4.17810587e-02 3.64978183e-03\n",
      " 7.13444156e-02 4.97281119e-02 6.55839738e-03 1.43972085e-03\n",
      " 1.63857177e-03 0.00000000e+00 7.31055953e-03 0.00000000e+00\n",
      " 1.23961989e-03 1.66677006e-01 2.08603207e-03 0.00000000e+00\n",
      " 2.61018114e-04 3.99939007e-02 0.00000000e+00 6.23634215e-02\n",
      " 3.46719886e-02 0.00000000e+00 2.77626134e-02 1.11690063e-01\n",
      " 8.51903363e-04 2.49095691e-03 5.29632485e-02 5.07780835e-04\n",
      " 6.05890646e-02 3.63420415e-03 2.12733050e-02 5.56490305e-02\n",
      " 2.00326736e-01 4.22594819e-02 2.81633731e-02 4.29013768e-02\n",
      " 1.13991386e-01 4.13207088e-02 7.13975204e-02 2.33909170e-02\n",
      " 2.75528921e-02 3.70844683e-02 4.09971696e-02 7.86067889e-02\n",
      " 4.90291624e-03 2.03480857e-03 1.44741952e-01 4.50496787e-03\n",
      " 1.02616965e-01 2.63585651e-02 2.45076029e-01 2.34294704e-01\n",
      " 9.21219772e-02 1.52672583e-01 3.99437481e-02 3.08532453e-02\n",
      " 2.45392584e-03 1.74277942e-03 2.80238349e-03 0.00000000e+00\n",
      " 9.31565947e-03 0.00000000e+00 3.06093313e-04 1.48081259e-02\n",
      " 8.39687689e-04 6.00273451e-02 3.68354887e-02 5.35156065e-04\n",
      " 2.93450649e-02 3.65290525e-02 0.00000000e+00 4.26405679e-03\n",
      " 2.99556267e-03 1.03597057e-01 3.43880357e-03 8.51515496e-03\n",
      " 2.56810447e-02 5.21532750e-02 1.07975091e-01 2.03919169e-02\n",
      " 1.81159194e-02 1.11944239e-02 2.61324453e-02 5.63404018e-02\n",
      " 2.72051461e-03 9.57092520e-02 1.44374377e-02 3.36286166e-03\n",
      " 4.22234272e-03 1.46951027e-01 2.34600193e-01 8.05610026e-02\n",
      " 8.73306546e-02 1.27535772e-01 3.63731497e-03 3.16787672e-02\n",
      " 5.30991180e-03 1.39296127e-01 0.00000000e+00 1.47344791e-02\n",
      " 5.91562829e-04 1.63009242e-01 1.92377556e-03 1.41734356e-02\n",
      " 6.94079299e-03 1.75120723e-01 0.00000000e+00 1.31920904e-02]\n",
      "model p-values None\n",
      "k best features are:  ['p_ethad', 'p_Rhad', 'p_Rhad1', 'p_ehad1', 'p_ethad1', 'p_deltaEta1', 'p_Reta', 'p_deltaEta2', 'p_Eratio', 'p_E7x11_Lr3', 'p_Rphi', 'p_E7x7_Lr3', 'p_deltaPhiRescaled2', 'p_E5x7_Lr3', 'p_f3core', 'p_e2tsts1', 'p_weta2', 'p_e2ts1', 'p_E3x5_Lr3', 'p_DeltaE']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['p_ethad',\n",
       " 'p_Rhad',\n",
       " 'p_Rhad1',\n",
       " 'p_ehad1',\n",
       " 'p_ethad1',\n",
       " 'p_deltaEta1',\n",
       " 'p_Reta',\n",
       " 'p_deltaEta2',\n",
       " 'p_Eratio',\n",
       " 'p_E7x11_Lr3',\n",
       " 'p_Rphi',\n",
       " 'p_E7x7_Lr3',\n",
       " 'p_deltaPhiRescaled2',\n",
       " 'p_E5x7_Lr3',\n",
       " 'p_f3core',\n",
       " 'p_e2tsts1',\n",
       " 'p_weta2',\n",
       " 'p_e2ts1',\n",
       " 'p_E3x5_Lr3',\n",
       " 'p_DeltaE']"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_feature_importance():\n",
    "    model = SelectKBest(mutual_info_classif, k=20)#选择k个最佳特征\n",
    "    X_new = model.fit_transform(X, y)\n",
    "    #feature_data是特征数据，label_data是标签数据，该函数可以选择出k个特征 \n",
    " \n",
    "    print(\"model shape: \",X_new.shape)\n",
    " \n",
    "    scores = model.scores_\n",
    "    print('model scores:', scores)  # 得分越高，特征越重要\n",
    " \n",
    "    p_values = model.pvalues_\n",
    "    print('model p-values', p_values)  # p-values 越小，置信度越高，特征越重要\n",
    " \n",
    "    # 按重要性排序，选出最重要的 k 个\n",
    "    indices = np.argsort(scores)[::-1]\n",
    "    k_best_features = list(X.columns.values[indices[0:20]])\n",
    " \n",
    "    print('k best features are: ',k_best_features)\n",
    "    \n",
    "    return k_best_features\n",
    "\n",
    "get_feature_importance()\n",
    "\n",
    "\n",
    "# def selection_features(X_train, y_train, X_test):\n",
    "#     select = SelectKBest(score_func=f_regression, k=20)\n",
    "#     select.fit(X_train, y_train)\n",
    "    \n",
    "#     return select\n",
    "\n",
    "# select = selection_features(X_train, y_train, X_test)\n",
    "# X = pd.DataFrame(X)\n",
    "# names = X.columns.values[select.get_support()]\n",
    "# scores = select.scores_[select.get_support()]\n",
    "# names_scores = list(zip(names, scores))\n",
    "# ns_df = pd.DataFrame(data = names_scores, columns=['Feature_names', 'Feature_scores'])\n",
    "# #Sort the dataframe for better visualization\n",
    "# ns_df_sorted = ns_df.sort_values(['Feature_names', 'Feature_scores'], ascending = [False, True])\n",
    "# print(ns_df_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature ranking by xgboost\n",
    "shap_variables = ['p_Rhad', 'p_Rphi', 'p_Reta', 'p_sigmad0', 'p_deltaEta1', 'p_ptconecoreTrackPtrCorrection', 'p_deltaPhiRescaled2', 'p_d0', 'p_numberOfInnermostPixelHits', 'p_ambiguityType',\n",
    "                    'p_rawPhiCluster','p_phiCalo', 'p_ethad', 'p_EptRatio', 'p_Rhad1', 'p_E7x11_Lr3', 'p_ehad1', 'p_Eratio', 'p_deltaPhi2', 'p_nTracks']\n",
    "\n",
    "# split data with shap\n",
    "X_shap = train[shap_variables]\n",
    "sc_X_shap = preprocessing.StandardScaler()\n",
    "X_shap_pre = sc_X_shap.fit_transform(X_shap)\n",
    "\n",
    "y_shap = train['Truth']\n",
    "X_shap_train, X_shap_test, y_shap_train, y_shap_test = train_test_split(X_shap, y_shap, test_size=0.2, random_state=12)\n",
    "X_shap_train_pre, X_shap_test_pre, y_shap_train_pre, y_shap_test_pre = train_test_split(X_shap_pre, y_shap, test_size=0.2, random_state=12)\n",
    "\n",
    "# feature ranking by lgb\n",
    "\n",
    "# non-preprocess\n",
    "lgb_variables = ['p_sigmad0', 'p_deltaPhiRescaled2', 'p_deltaEta1', 'p_Reta', 'p_d0', 'p_ptconecoreTrackPtrCorrection',\n",
    "'p_d0Sig', 'p_Rhad', 'p_Rphi', 'p_deltaPhiFromLastMeasurement', 'p_deltaPhi2', 'p_EptRatio', 'p_dPOverP', 'p_numberOfSCTHits',\n",
    " 'p_numberOfPixelHits', 'p_pt_track', 'p_nTracks', 'p_ethad', 'p_qOverP','p_ambiguityType']\n",
    "\n",
    "\n",
    "X_lgb = train[lgb_variables]\n",
    "sc_X_lgb = preprocessing.StandardScaler()\n",
    "X_lgb_pre = sc_X_lgb.fit_transform(X_lgb)\n",
    "\n",
    "y_lgb = train['Truth']\n",
    "X_lgb_train, X_lgb_test, y_lgb_train, y_lgb_test = train_test_split(X_lgb, y_lgb, test_size=0.2, random_state=12)\n",
    "X_lgb_train_pre, X_lgb_test_pre, y_lgb_train_pre, y_lgb_test_pre = train_test_split(X_lgb_pre, y_lgb, test_size=0.2, random_state=12)\n",
    "\n",
    "# feature ranking by kbest\n",
    "kbest_variables = ['p_ethad', 'p_Rhad', 'p_Rhad1', 'p_ehad1', 'p_ethad1', 'p_deltaEta1', 'p_Reta', 'p_deltaEta2', 'p_Eratio', 'p_E7x11_Lr3', 'p_Rphi', 'p_E7x7_Lr3', \n",
    "'p_deltaPhiRescaled2', 'p_E5x7_Lr3', 'p_f3core', 'p_e2tsts1', 'p_e2ts1', 'p_weta2', 'p_E3x5_Lr3', 'p_DeltaE']\n",
    "\n",
    "X_kbest = train[kbest_variables]\n",
    "sc_X_kbest = preprocessing.StandardScaler()\n",
    "X_kbest_pre = sc_X_lgb.fit_transform(X_kbest)\n",
    "\n",
    "y_kbest = train['Truth']\n",
    "X_kbest_train, X_kbest_test, y_kbest_train, y_kbest_test = train_test_split(X_kbest, y_kbest, test_size=0.2, random_state=12)\n",
    "X_kbest_train_pre, X_kbest_test_pre, y_kbest_train_pre, y_kbest_test_pre = train_test_split(X_kbest_pre, y_kbest, test_size=0.2, random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna.integration.lightgbm as oplgb\n",
    "import optuna\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "\n",
    "\n",
    "X_lgb_train, X_lgb_test, y_lgb_train, y_lgb_test = train_test_split(X_lgb, y_lgb, test_size=0.2, random_state=42)\n",
    "\n",
    "def objective_lgb():\n",
    "    \n",
    "    train_data = lgb.Dataset(X_lgb_train, label=y_lgb_train)\n",
    "    valid_data = lgb.Dataset(X_lgb_test, label=y_lgb_test)\n",
    "    \n",
    "    rkf = RepeatedKFold(n_splits=10, n_repeats=10, random_state=42)\n",
    "    params = {\n",
    "            'objective': 'regression',\n",
    "            'metric': 'binary_logloss',\n",
    "            'boosting_type': 'dart',\n",
    "            'max_depth': 10,\n",
    "            'learning_rate': 0.2759844445088989,\n",
    "            'feature_fraction': 0.8,\n",
    "            'num_leaves': 360,\n",
    "            'bagging_freq': 1,\n",
    "            'bagging_fraction': 0.8,\n",
    "            'reg_alpha': 8.200000000000001,\n",
    "            'reg_lambda': 103,\n",
    "            'verbose':-1,\n",
    "            'force_col_wise': True\n",
    "    }\n",
    "\n",
    "    study_tuner = optuna.create_study(\n",
    "    direction=\"minimize\")\n",
    "\n",
    "    tuner = oplgb.LightGBMTunerCV(params, train_data, study=study_tuner, num_boost_round=100, folds=rkf, \n",
    "                                early_stopping_rounds=200, seed=42)\n",
    "\n",
    "    tuner.run()\n",
    "\n",
    "    return tuner.best_params\n",
    "\n",
    "# {'objective': 'regression',\n",
    "#  'metric': 'binary_logloss',\n",
    "#  'boosting_type': 'dart',\n",
    "#  'max_depth': 10,\n",
    "#  'learning_rate': 0.2759844445088989,\n",
    "#  'feature_fraction': 0.9520000000000001,\n",
    "#  'num_leaves': 360,\n",
    "#  'bagging_freq': 1,\n",
    "#  'bagging_fraction': 0.9993807771263824,\n",
    "#  'verbose': -1,\n",
    "#  'force_col_wise': True,\n",
    "#  'feature_pre_filter': False,\n",
    "#  'lambda_l1': 8.200000000000001,\n",
    "#  'lambda_l2': 103,\n",
    "#  'min_child_samples': 20}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimized_lgb(X_train, X_test, y_train, y_test):\n",
    "    train_data = lgb.Dataset(X_train, label=y_train)\n",
    "    test_data = lgb.Dataset(X_test, label=y_test)\n",
    "    params = {'objective': 'regression',\n",
    "            'metric': 'binary_logloss',\n",
    "            'boosting_type': 'dart',\n",
    "            'max_depth': 10,\n",
    "            'learning_rate': 0.2759844445088989,\n",
    "            'feature_fraction': 0.8,\n",
    "            'num_leaves': 360,\n",
    "            'bagging_freq': 1,\n",
    "            'bagging_fraction': 0.8,\n",
    "            'reg_alpha': 8.200000000000001,\n",
    "            'reg_lambda': 103,\n",
    "            'verbose':-1,\n",
    "            'force_col_wise': True}\n",
    "\n",
    "    lgb_clf = lgb.train(params, train_set=train_data, num_boost_round=1000)\n",
    "    y_pred = np.around(lgb_clf.predict(X.values))\n",
    "    acc = accuracy_score(y_pred, y)\n",
    "    print(f\"Train accuracy: {acc*100.0:.2f}%\")\n",
    "\n",
    "    return lgb_clf.predict(X.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimize hyperparameters of lgb\n",
    "# train_data = lgb.Dataset(X_train, label=y_train)\n",
    "# valid_data = lgb.Dataset(X_test, label=y_test)\n",
    "\n",
    "# def objective_lgb(trial):\n",
    "    \n",
    "#     train_data = lgb.Dataset(X_train, label=y_train)\n",
    "#     valid_data = lgb.Dataset(X_test, label=y_test)\n",
    "\n",
    "#     boosting_types = [\"gbdt\", \"rf\", \"dart\"]\n",
    "#     boosting_type = trial.suggest_categorical(\"boosting_type\", boosting_types)\n",
    "\n",
    "#     params = {\n",
    "#         'objective': 'regression',\n",
    "#         'boosting_type': trial.suggest_categorical(\"boosting_type\", [\"gbdt\", \"rf\", \"dart\"]),\n",
    "#         'max_depth': trial.suggest_int('max_depth', 2, 12),\n",
    "#         'metric': {'l2', 'auc'},\n",
    "#         'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.5),\n",
    "#         'feature_fraction': trial.suggest_float('feature_fraction', 0.2, 0.95, step=0.1),\n",
    "#         'num_leaves': trial.suggest_int('num_leaves', 20, 3000, step=20),\n",
    "#         'bagging_freq': trial.suggest_categorical('bagging_freq', [1]),\n",
    "#         'bagging_fraction': trial.suggest_float('bagging_fraction', 0.2, 0.95, step=0.1),\n",
    "#         'reg_alpha': trial.suggest_float(\"reg_alpha\", 0, 100, step=0.1),\n",
    "#         'reg_lambda': trial.suggest_int(\"reg_lambda\", 0, 1000, step=1),\n",
    "#         'verbosity': -1,\n",
    "#     }\n",
    "\n",
    "#     N_iterations_max = 10000\n",
    "#     early_stopping_rounds = 50\n",
    "\n",
    "#     if boosting_type == \"dart\":\n",
    "#         N_iterations_max = 100\n",
    "#         early_stopping_rounds = None\n",
    "\n",
    "#     cv_res = lgb.cv(\n",
    "#         params,\n",
    "#         train_data,\n",
    "#         num_boost_round=N_iterations_max,\n",
    "#         early_stopping_rounds=early_stopping_rounds,\n",
    "#         verbose_eval=False,\n",
    "#         seed=42,\n",
    "#         callbacks=[LightGBMPruningCallback(trial, \"binary_logloss\")],\n",
    "#     )\n",
    "\n",
    "#     num_boost_round = len(cv_res[\"auc-mean\"])\n",
    "#     trial.set_user_attr(\"num_boost_round\", num_boost_round)\n",
    "\n",
    "#     return cv_res[\"auc-mean\"][-1]\n",
    "\n",
    "\n",
    "\n",
    "# study = optuna.create_study(\n",
    "#     direction=\"maximize\",\n",
    "#     sampler=TPESampler(seed=42),\n",
    "#     pruner=MedianPruner(n_warmup_steps=50),\n",
    "# )\n",
    "\n",
    "# study.optimize(objective_lgb, n_trials=100, show_progress_bar=True)\n",
    "\n",
    "# study.best_trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimized lgb\n",
    "\n",
    "def optimized_lgb(X_train, X_test, y_train, y_test):\n",
    "    train_data = lgb.Dataset(X_train, label=y_train)\n",
    "    test_data = lgb.Dataset(X_test, label=y_test)\n",
    "    params = {'objective': 'regression',\n",
    "            'metric': 'binary_logloss',\n",
    "            'boosting_type': 'dart',\n",
    "            'max_depth': 10,\n",
    "            'learning_rate': 0.2759844445088989,\n",
    "            'feature_fraction': 0.8,\n",
    "            'num_leaves': 360,\n",
    "            'bagging_freq': 1,\n",
    "            'bagging_fraction': 0.8,\n",
    "            'reg_alpha': 8.200000000000001,\n",
    "            'reg_lambda': 103,\n",
    "            'verbose':-1,\n",
    "            'force_col_wise': True}\n",
    "\n",
    "    lgb_clf = lgb.train(params, train_set=train_data, num_boost_round=1000)\n",
    "    y_pred = np.around(lgb_clf.predict(X.values))\n",
    "    acc = accuracy_score(y_pred, y)\n",
    "    print(f\"Train accuracy: {acc*100.0:.2f}%\")\n",
    "\n",
    "    return lgb_clf.predict(X.values), acc\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'boosting_type': 'dart',\n",
    " 'max_depth': 10,\n",
    " 'learning_rate': 0.2759844445088989,\n",
    " 'feature_fraction': 0.8,\n",
    " 'num_leaves': 360,\n",
    " 'bagging_freq': 1,\n",
    " 'bagging_fraction': 0.8,\n",
    " 'reg_alpha': 8.200000000000001,\n",
    " 'reg_lambda': 103}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network\n",
    "\n",
    "import kerastuner as kt\n",
    "\n",
    "# class ClearTrainingOutput(tf.keras.callbacks.Callback):\n",
    "#     def on_train_end(*args, **kwargs):\n",
    "#         print(\"训练完成，调用回调方法\")\n",
    "\n",
    "# def model_builder(hp):\n",
    "#     model = Sequential()\n",
    "#     # Tune the number of units in the first Dense layer\n",
    "#     # Choose an optimal value between 32-512\n",
    "#     hp_units = hp.Int('units', min_value=32, max_value=512, step=32)\n",
    "#     model.add(Dense(units=hp_units, activation='relu'))\n",
    "#     model.add(Dense(units=hp_units, activation='relu'))\n",
    "#     model.add(Dense(units=1, activation='relu'))\n",
    "\n",
    "#     # Tune the learning rate for the optimizer\n",
    "#     # Choose an optimal value from 0.01, 0.001, or 0.0001\n",
    "#     hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "\n",
    "#     model.compile(optimizer=Adam(learning_rate=hp_learning_rate),\n",
    "#                   loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "#                   metrics=['accuracy'])  # accuracy，用于判断模型效果的函数\n",
    "#     return model\n",
    "    \n",
    "# tuner = kt.Hyperband(model_builder,\n",
    "#                     objective='val_accuracy',  # 优化的目标，验证集accuracy\n",
    "#                     max_epochs=10,  # 最大迭代次数\n",
    "#                     factor=3)\n",
    "\n",
    "# tuner.search(X_train, y_train, epochs=10, validation_data=(X_test, y_test),\n",
    "#                  callbacks=[ClearTrainingOutput()])\n",
    "\n",
    "# tuner.get_best_hyperparameters(num_trials=1)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext tensorboard\n",
    "from tensorboard.plugins.hparams import api as hp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter tuning for neural network\n",
    "\n",
    "# HP_NUM_UNITS1 = hp.HParam('num_units1', hp.Discrete([16, 32, 64, 128]))\n",
    "# HP_NUM_UNITS2 = hp.HParam('num_units2', hp.Discrete([16, 32, 64, 128]))\n",
    "# HP_NUM_UNITS3 = hp.HParam('num_units3', hp.Discrete([16, 32, 64, 128]))\n",
    "# HP_DROPOUT = hp.HParam('dropout', hp.RealInterval(0.1, 0.2))\n",
    "\n",
    "# METRIC_ACCURACY = 'accuracy'\n",
    "\n",
    "# with tf.summary.create_file_writer('logs/hparam_tuning').as_default():\n",
    "#   hp.hparams_config(\n",
    "#     hparams=[HP_NUM_UNITS1, HP_NUM_UNITS2, HP_NUM_UNITS3],\n",
    "#     metrics=[hp.Metric(METRIC_ACCURACY, display_name='Accuracy')],\n",
    "#   )\n",
    "\n",
    "# def train_test_model(hparams):\n",
    "#   model = tf.keras.models.Sequential([\n",
    "#     tf.keras.layers.Dense(hparams[HP_NUM_UNITS1], activation=tf.nn.relu),\n",
    "#     tf.keras.layers.Dense(hparams[HP_NUM_UNITS2], activation=tf.nn.relu),\n",
    "#     tf.keras.layers.Dense(hparams[HP_NUM_UNITS3], activation=tf.nn.relu),\n",
    "#     tf.keras.layers.Dense(1),\n",
    "#   ])\n",
    "#   model.compile(\n",
    "#       optimizer='adam',\n",
    "#       loss='mae',\n",
    "#       metrics=['accuracy'],\n",
    "#   )\n",
    "\n",
    "#   model.fit(X_train, y_train, epochs=5) # Run with 1 epoch to speed things up for demo purposes\n",
    "#   _, accuracy = model.evaluate(X_test, y_test)\n",
    "#   return accuracy\n",
    "\n",
    "# def run(run_dir, hparams):\n",
    "#   with tf.summary.create_file_writer(run_dir).as_default():\n",
    "#     hp.hparams(hparams)  # record the values used in this trial\n",
    "#     accuracy = train_test_model(hparams)\n",
    "#     tf.summary.scalar(METRIC_ACCURACY, accuracy, step=1)\n",
    "\n",
    "# session_num = 0\n",
    "\n",
    "# for num_units1 in HP_NUM_UNITS1.domain.values:\n",
    "#     for num_units2 in HP_NUM_UNITS2.domain.values:\n",
    "#       for num_units3 in HP_NUM_UNITS3.domain.values:\n",
    "#             hparams = {\n",
    "#                 HP_NUM_UNITS1: num_units1,\n",
    "#                 HP_NUM_UNITS2: num_units2,\n",
    "#                 HP_NUM_UNITS3: num_units3\n",
    "#             }\n",
    "#             run_name = \"run-%d\" % session_num\n",
    "#             print('--- Starting trial: %s' % run_name)\n",
    "#             print({h.name: hparams[h] for h in hparams})\n",
    "#             run('logs/hparam_tuning/' + run_name, hparams)\n",
    "#             session_num += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %tensorboard --logdir logs/hparam_tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- TRAINING ---------\n",
      "Epoch 1/5\n",
      "4063/4063 [==============================] - 3s 744us/step - loss: 0.3737 - Accuracy: 0.8676 - val_loss: 0.2796 - val_Accuracy: 0.9132\n",
      "Epoch 2/5\n",
      "4063/4063 [==============================] - 3s 732us/step - loss: 0.3097 - Accuracy: 0.8902 - val_loss: 0.2447 - val_Accuracy: 0.9208\n",
      "Epoch 3/5\n",
      "4063/4063 [==============================] - 3s 728us/step - loss: 0.2954 - Accuracy: 0.9124 - val_loss: 0.3151 - val_Accuracy: 0.9165\n",
      "Epoch 4/5\n",
      "4063/4063 [==============================] - 3s 731us/step - loss: 0.2601 - Accuracy: 0.9185 - val_loss: 0.2432 - val_Accuracy: 0.9199\n",
      "Epoch 5/5\n",
      "4063/4063 [==============================] - 3s 729us/step - loss: 0.2529 - Accuracy: 0.9192 - val_loss: 0.2396 - val_Accuracy: 0.9199\n",
      "1016/1016 - 0s - loss: 0.2396 - Accuracy: 0.9199 - 427ms/epoch - 420us/step\n",
      "1016/1016 [==============================] - 0s 420us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.2395523339509964, 0.9198769330978394],\n",
       " array([[0.71966153],\n",
       "        [0.92709726],\n",
       "        [0.8898367 ],\n",
       "        ...,\n",
       "        [0.93942344],\n",
       "        [0.7674445 ],\n",
       "        [0.30673143]], dtype=float32))"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def optimized_nn(X_train1, y_train1, X_test1, y_test1):\n",
    "    model = Sequential([\n",
    "        Dense(9,activation='relu',name='input_layer'),\n",
    "        Dense(24,activation='relu',name='hidden_layer1'),\n",
    "        Dense(12,activation='relu',name='hidden_layer2'),\n",
    "        Dense(1, name='output')])\n",
    "\n",
    "   \n",
    "    model.compile(optimizer='adam',\n",
    "                loss='BinaryCrossentropy',\n",
    "                metrics='Accuracy')\n",
    "\n",
    "    print('--------- TRAINING ---------')\n",
    "    history = model.fit(x=X_train1, y=y_train1, validation_data=(X_test1, y_test1), epochs = 5)  \n",
    "    score = model.evaluate(X_test1,  y_test1, verbose=2)\n",
    "    y_pred = model.predict(X_test1)\n",
    "    # y_pred = sc_y.inverse_transform(y_pred)\n",
    "\n",
    "    return score, y_pred\n",
    "\n",
    "optimized_nn(X_kbest_train_pre, y_kbest_train_pre, X_kbest_test_pre, y_kbest_test_pre)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4e3119b6b440005e83014445b502bc062a01c9850f9c4ea1b0d68db6d948f423"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
