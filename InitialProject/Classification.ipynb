{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 759,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import pandas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import tree\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "import shap\n",
    "import lightgbm as lgb \n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from optuna.integration import LightGBMPruningCallback\n",
    "from optuna.pruners import MedianPruner\n",
    "import xgboost\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow import keras\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from tensorflow.keras.metrics import Accuracy, BinaryCrossentropy\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, chi2, f_classif, mutual_info_regression, mutual_info_classif, VarianceThreshold\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 760,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training data set: (162500, 166)\n",
      "Shape of test data set: (160651, 164)\n",
      "Shape of X: (162500, 160)\n",
      "Shape of y: (162500,)\n",
      "41005\n",
      "121495\n"
     ]
    }
   ],
   "source": [
    "# read data\n",
    "def load_data(name):\n",
    "    with h5py.File(f'{name}.h5', 'r') as f:\n",
    "        filename = name.split('/')[-1]\n",
    "        return pandas.DataFrame(f[filename][:], dtype=np.float64)\n",
    "\n",
    "train = load_data('train')\n",
    "test  = load_data('test')\n",
    "\n",
    "print (f'Shape of training data set: {train.shape}')\n",
    "print (f'Shape of test data set: {test.shape}')\n",
    "\n",
    "all_variables = ['actualInteractionsPerCrossing', 'averageInteractionsPerCrossing', 'correctedActualMu', 'correctedAverageMu', 'correctedScaledActualMu', 'correctedScaledAverageMu', 'NvtxReco', 'p_nTracks', 'p_pt_track', 'p_eta', 'p_phi', 'p_charge', 'p_qOverP', 'p_z0', 'p_d0', 'p_sigmad0', 'p_d0Sig', 'p_EptRatio', 'p_dPOverP', 'p_z0theta', 'p_etaCluster', 'p_phiCluster', 'p_eCluster', 'p_rawEtaCluster', 'p_rawPhiCluster', 'p_rawECluster', 'p_eClusterLr0', 'p_eClusterLr1', 'p_eClusterLr2', 'p_eClusterLr3', 'p_etaClusterLr1', 'p_etaClusterLr2', 'p_phiClusterLr2', 'p_eAccCluster', 'p_f0Cluster', 'p_etaCalo', 'p_phiCalo', 'p_eTileGap3Cluster', 'p_cellIndexCluster', 'p_phiModCalo', 'p_etaModCalo', 'p_dPhiTH3', 'p_R12', 'p_fTG3', 'p_weta2', 'p_Reta', 'p_Rphi', 'p_Eratio', 'p_f1', 'p_f3', 'p_Rhad', 'p_Rhad1', 'p_deltaEta1', 'p_deltaPhiRescaled2', 'p_TRTPID', 'p_TRTTrackOccupancy', 'p_numberOfInnermostPixelHits', 'p_numberOfPixelHits', 'p_numberOfSCTHits', 'p_numberOfTRTHits', 'p_numberOfTRTXenonHits', 'p_chi2', 'p_ndof', 'p_SharedMuonTrack', 'p_E7x7_Lr2', 'p_E7x7_Lr3', 'p_E_Lr0_HiG', 'p_E_Lr0_LowG', 'p_E_Lr0_MedG', 'p_E_Lr1_HiG', 'p_E_Lr1_LowG', 'p_E_Lr1_MedG', 'p_E_Lr2_HiG', 'p_E_Lr2_LowG', 'p_E_Lr2_MedG', 'p_E_Lr3_HiG', 'p_E_Lr3_LowG', 'p_E_Lr3_MedG', 'p_ambiguityType', 'p_asy1', 'p_author', 'p_barys1', 'p_core57cellsEnergyCorrection', 'p_deltaEta0', 'p_deltaEta2', 'p_deltaEta3', 'p_deltaPhi0', 'p_deltaPhi1', 'p_deltaPhi2', 'p_deltaPhi3', 'p_deltaPhiFromLastMeasurement', 'p_deltaPhiRescaled0', 'p_deltaPhiRescaled1', 'p_deltaPhiRescaled3', 'p_e1152', 'p_e132', 'p_e235', 'p_e255', 'p_e2ts1', 'p_ecore', 'p_emins1', 'p_etconeCorrBitset', 'p_ethad', 'p_ethad1', 'p_f1core', 'p_f3core', 'p_maxEcell_energy', 'p_maxEcell_gain', 'p_maxEcell_time', 'p_maxEcell_x', 'p_maxEcell_y', 'p_maxEcell_z', 'p_nCells_Lr0_HiG', 'p_nCells_Lr0_LowG', 'p_nCells_Lr0_MedG', 'p_nCells_Lr1_HiG', 'p_nCells_Lr1_LowG', 'p_nCells_Lr1_MedG', 'p_nCells_Lr2_HiG', 'p_nCells_Lr2_LowG', 'p_nCells_Lr2_MedG', 'p_nCells_Lr3_HiG', 'p_nCells_Lr3_LowG', 'p_nCells_Lr3_MedG', 'p_pos', 'p_pos7', 'p_poscs1', 'p_poscs2', 'p_ptconeCorrBitset', 'p_ptconecoreTrackPtrCorrection', 'p_r33over37allcalo', 'p_topoetconeCorrBitset', 'p_topoetconecoreConeEnergyCorrection', 'p_topoetconecoreConeSCEnergyCorrection', 'p_weta1', 'p_widths1', 'p_widths2', 'p_wtots1', 'p_e233', 'p_e237', 'p_e277', 'p_e2tsts1', 'p_ehad1', 'p_emaxs1', 'p_fracs1', 'p_DeltaE', 'p_E3x5_Lr0', 'p_E3x5_Lr1', 'p_E3x5_Lr2', 'p_E3x5_Lr3', 'p_E5x7_Lr0', 'p_E5x7_Lr1', 'p_E5x7_Lr2', 'p_E5x7_Lr3', 'p_E7x11_Lr0', 'p_E7x11_Lr1', 'p_E7x11_Lr2', 'p_E7x11_Lr3', 'p_E7x7_Lr0', 'p_E7x7_Lr1' ]\n",
    "\n",
    "\n",
    "X = train[all_variables]\n",
    "sc_X = preprocessing.StandardScaler()\n",
    "X = sc_X.fit_transform(X)\n",
    "y = train['Truth']\n",
    "\n",
    "X = pd.DataFrame(X, columns=all_variables)\n",
    "\n",
    "print (f'Shape of X: {X.shape}')\n",
    "print (f'Shape of y: {y.shape}')\n",
    "\n",
    "print(sum(y==0))\n",
    "print(sum(y==1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 761,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 762,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shap value xgbooster\n",
    "\n",
    "def shap_xgbooster():\n",
    "    model = xgboost.XGBRegressor().fit(X, y)\n",
    "    explainer = shap.Explainer(model)\n",
    "    shap_values = explainer(X)\n",
    "\n",
    "    feature_names = shap_values.feature_names\n",
    "    shap_df = pd.DataFrame(shap_values.values, columns=feature_names)\n",
    "    vals = np.abs(shap_df.values).mean(0)\n",
    "    shap_importance = pd.DataFrame(list(zip(feature_names, vals)), columns=['col_name', 'feature_importance_vals'])\n",
    "    shap_importance.sort_values(by=['feature_importance_vals'], ascending=False, inplace=True)\n",
    "    shap.plots.bar(shap_values)\n",
    "\n",
    "    return shap_importance.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 763,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cols</th>\n",
       "      <th>feature_imp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>p_d0</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>p_deltaPhiRescaled2</td>\n",
       "      <td>138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>p_ptconecoreTrackPtrCorrection</td>\n",
       "      <td>129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>p_deltaEta1</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>p_Reta</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>p_sigmad0</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>p_Rphi</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>p_d0Sig</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>p_Rhad</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>p_deltaPhiFromLastMeasurement</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>p_EptRatio</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>p_deltaPhi2</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>p_dPOverP</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>p_numberOfSCTHits</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>p_numberOfPixelHits</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>p_nTracks</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>p_pt_track</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>p_ambiguityType</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>p_qOverP</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>p_ethad</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               cols  feature_imp\n",
       "14                             p_d0          141\n",
       "53              p_deltaPhiRescaled2          138\n",
       "129  p_ptconecoreTrackPtrCorrection          129\n",
       "52                      p_deltaEta1          124\n",
       "45                           p_Reta          123\n",
       "15                        p_sigmad0          121\n",
       "46                           p_Rphi          106\n",
       "16                          p_d0Sig           90\n",
       "50                           p_Rhad           86\n",
       "90    p_deltaPhiFromLastMeasurement           83\n",
       "17                       p_EptRatio           80\n",
       "88                      p_deltaPhi2           77\n",
       "18                        p_dPOverP           64\n",
       "58                p_numberOfSCTHits           60\n",
       "57              p_numberOfPixelHits           56\n",
       "7                         p_nTracks           54\n",
       "8                        p_pt_track           51\n",
       "78                  p_ambiguityType           46\n",
       "12                         p_qOverP           46\n",
       "102                         p_ethad           43"
      ]
     },
     "execution_count": 763,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# feature ranking lgb\n",
    "\n",
    "def feature_importance_lgb():\n",
    "    gbm = lgb.LGBMRegressor()\n",
    "    gbm.fit(X_train, y_train)\n",
    "    gbm.booster_.feature_importance()\n",
    "\n",
    "    feature_imp_ = pd.DataFrame({'cols':X_train.columns, 'feature_imp':gbm.feature_importances_})\n",
    "    feature_imp_.loc[feature_imp_.feature_imp > 0].sort_values(by=['feature_imp'], ascending=False)\n",
    "\n",
    "    return feature_imp_.loc[feature_imp_.feature_imp > 0].sort_values(by=['feature_imp'], ascending=False).head(20)\n",
    "\n",
    "feature_importance_lgb()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 764,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_feature_importance():\n",
    "#     model = SelectKBest(mutual_info_classif, k=20)#选择k个最佳特征\n",
    "#     X_new = model.fit_transform(X, y)\n",
    "#     #feature_data是特征数据，label_data是标签数据，该函数可以选择出k个特征 \n",
    " \n",
    "#     print(\"model shape: \",X_new.shape)\n",
    " \n",
    "#     scores = model.scores_\n",
    "#     print('model scores:', scores)  # 得分越高，特征越重要\n",
    " \n",
    "#     p_values = model.pvalues_\n",
    "#     print('model p-values', p_values)  # p-values 越小，置信度越高，特征越重要\n",
    " \n",
    "#     # 按重要性排序，选出最重要的 k 个\n",
    "#     indices = np.argsort(scores)[::-1]\n",
    "#     k_best_features = list(X.columns.values[indices[0:20]])\n",
    " \n",
    "#     print('k best features are: ',k_best_features)\n",
    "    \n",
    "#     return k_best_features\n",
    "\n",
    "# get_feature_importance()\n",
    "\n",
    "\n",
    "# def selection_features(X_train, y_train, X_test):\n",
    "#     select = SelectKBest(score_func=f_regression, k=20)\n",
    "#     select.fit(X_train, y_train)\n",
    "    \n",
    "#     return select\n",
    "\n",
    "# select = selection_features(X_train, y_train, X_test)\n",
    "# X = pd.DataFrame(X)\n",
    "# names = X.columns.values[select.get_support()]\n",
    "# scores = select.scores_[select.get_support()]\n",
    "# names_scores = list(zip(names, scores))\n",
    "# ns_df = pd.DataFrame(data = names_scores, columns=['Feature_names', 'Feature_scores'])\n",
    "# #Sort the dataframe for better visualization\n",
    "# ns_df_sorted = ns_df.sort_values(['Feature_names', 'Feature_scores'], ascending = [False, True])\n",
    "# print(ns_df_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 765,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature ranking by xgboost\n",
    "shap_variables = ['p_Rhad', 'p_Rphi', 'p_Reta', 'p_sigmad0', 'p_deltaEta1', 'p_ptconecoreTrackPtrCorrection', 'p_deltaPhiRescaled2', 'p_d0', 'p_numberOfInnermostPixelHits', 'p_ambiguityType',\n",
    "                    'p_rawPhiCluster','p_phiCalo', 'p_ethad', 'p_EptRatio', 'p_Rhad1', 'p_E7x11_Lr3', 'p_ehad1', 'p_Eratio', 'p_deltaPhi2', 'p_nTracks']\n",
    "\n",
    "# split data with shap\n",
    "X_shap = train[shap_variables]\n",
    "sc_X_shap = preprocessing.StandardScaler()\n",
    "X_shap_pre = sc_X_shap.fit_transform(X_shap)\n",
    "\n",
    "y_shap = train['Truth']\n",
    "X_shap_train, X_shap_test, y_shap_train, y_shap_test = train_test_split(X_shap, y_shap, test_size=0.2, random_state=12)\n",
    "X_shap_train_pre, X_shap_test_pre, y_shap_train_pre, y_shap_test_pre = train_test_split(X_shap_pre, y_shap, test_size=0.2, random_state=12)\n",
    "\n",
    "sc_input_shap = preprocessing.StandardScaler()\n",
    "input_valid_shap = test[shap_variables]\n",
    "input_valid_shap = sc_input_shap.fit_transform(input_valid_shap)\n",
    "input_valid_shap = pd.DataFrame(input_valid_shap, columns=[shap_variables])\n",
    "\n",
    "\n",
    "# feature ranking by lgb\n",
    "\n",
    "# non-preprocess\n",
    "lgb_variables = ['p_sigmad0', 'p_deltaPhiRescaled2', 'p_deltaEta1', 'p_Reta', 'p_d0', 'p_ptconecoreTrackPtrCorrection',\n",
    "'p_d0Sig', 'p_Rhad', 'p_Rphi', 'p_deltaPhiFromLastMeasurement', 'p_deltaPhi2', 'p_EptRatio', 'p_dPOverP', 'p_numberOfSCTHits',\n",
    " 'p_numberOfPixelHits', 'p_pt_track', 'p_nTracks', 'p_ethad', 'p_qOverP','p_ambiguityType']\n",
    "\n",
    "\n",
    "X_lgb = train[lgb_variables]\n",
    "sc_X_lgb = preprocessing.StandardScaler()\n",
    "X_lgb_pre = sc_X_lgb.fit_transform(X_lgb)\n",
    "\n",
    "y_lgb = train['Truth']\n",
    "X_lgb_train, X_lgb_test, y_lgb_train, y_lgb_test = train_test_split(X_lgb, y_lgb, test_size=0.2, random_state=12)\n",
    "X_lgb_train_pre, X_lgb_test_pre, y_lgb_train_pre, y_lgb_test_pre = train_test_split(X_lgb_pre, y_lgb, test_size=0.2, random_state=12)\n",
    "\n",
    "X_lgb_train, X_lgb_test, y_lgb_train, y_lgb_test = train_test_split(X_lgb, y_lgb, test_size=0.2, random_state=12)\n",
    "sc_input_lgb = preprocessing.StandardScaler()\n",
    "input_valid_lgb = test[lgb_variables]\n",
    "input_valid_lgb = sc_input_lgb.fit_transform(input_valid_lgb)\n",
    "input_valid_lgb = pd.DataFrame(input_valid_lgb, columns=[lgb_variables])\n",
    "\n",
    "# feature ranking by kbest\n",
    "kbest_variables = ['p_ethad', 'p_Rhad', 'p_Rhad1', 'p_ehad1', 'p_ethad1', 'p_deltaEta1', 'p_Reta', 'p_deltaEta2', 'p_Eratio', 'p_E7x11_Lr3', 'p_Rphi', 'p_E7x7_Lr3', \n",
    "'p_deltaPhiRescaled2', 'p_E5x7_Lr3', 'p_f3core', 'p_e2tsts1', 'p_e2ts1', 'p_weta2', 'p_E3x5_Lr3', 'p_DeltaE']\n",
    "\n",
    "X_kbest = train[kbest_variables]\n",
    "sc_X_kbest = preprocessing.StandardScaler()\n",
    "X_kbest_pre = sc_X_lgb.fit_transform(X_kbest)\n",
    "\n",
    "y_kbest = train['Truth']\n",
    "X_kbest_train, X_kbest_test, y_kbest_train, y_kbest_test = train_test_split(X_kbest, y_kbest, test_size=0.2, random_state=12)\n",
    "X_kbest_train_pre, X_kbest_test_pre, y_kbest_train_pre, y_kbest_test_pre = train_test_split(X_kbest_pre, y_kbest, test_size=0.2, random_state=12)\n",
    "\n",
    "sc_input_kbest = preprocessing.StandardScaler()\n",
    "input_valid_kbest= test[kbest_variables]\n",
    "input_valid_kbest = sc_input_kbest.fit_transform(input_valid_kbest)\n",
    "input_valid_kbest = pd.DataFrame(input_valid_kbest, columns=[kbest_variables])\n",
    "\n",
    "variable_list_shap = pd.DataFrame(shap_variables, columns=['vars'])\n",
    "variable_list_shap.to_csv('solutions/Classification_HauLamFong_xgboost_VariableList.txt')\n",
    "variable_list_lgb = pd.DataFrame(lgb_variables, columns=['vars'])\n",
    "variable_list_lgb.to_csv('solutions/Classification_HauLamFong_lightgbm_VariableList.txt')\n",
    "variable_list_kbest = pd.DataFrame(kbest_variables, columns=['vars'])\n",
    "variable_list_kbest.to_csv('solutions/Classification_HauLamFong_neuralnetwork_VariableList.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 766,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna.integration.lightgbm as oplgb\n",
    "import optuna\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "\n",
    "\n",
    "# def objective_lgb():\n",
    "    \n",
    "#     train_data = lgb.Dataset(X_lgb_train, label=y_lgb_train)\n",
    "#     valid_data = lgb.Dataset(X_lgb_test, label=y_lgb_test)\n",
    "    \n",
    "#     rkf = RepeatedKFold(n_splits=10, n_repeats=10, random_state=42)\n",
    "#     params = {\n",
    "#             'objective': 'regression',\n",
    "#             'metric': 'binary_logloss',\n",
    "#             'boosting_type': 'dart',\n",
    "#             'max_depth': 10,\n",
    "#             'learning_rate': 0.2759844445088989,\n",
    "#             'feature_fraction': 0.8,\n",
    "#             'num_leaves': 360,\n",
    "#             'bagging_freq': 1,\n",
    "#             'bagging_fraction': 0.8,\n",
    "#             'reg_alpha': 8.200000000000001,\n",
    "#             'reg_lambda': 103,\n",
    "#             'verbose':-1,\n",
    "#             'force_col_wise': True\n",
    "#     }\n",
    "\n",
    "#     study_tuner = optuna.create_study(\n",
    "#     direction=\"minimize\")\n",
    "\n",
    "#     tuner = oplgb.LightGBMTunerCV(params, train_data, study=study_tuner, num_boost_round=100, folds=rkf, \n",
    "#                                 early_stopping_rounds=200, seed=42)\n",
    "\n",
    "#     tuner.run()\n",
    "\n",
    "#     return tuner.best_params\n",
    "\n",
    "# {'objective': 'regression',\n",
    "#  'metric': 'binary_logloss',\n",
    "#  'boosting_type': 'dart',\n",
    "#  'max_depth': 10,\n",
    "#  'learning_rate': 0.2759844445088989,\n",
    "#  'feature_fraction': 0.9520000000000001,\n",
    "#  'num_leaves': 360,\n",
    "#  'bagging_freq': 1,\n",
    "#  'bagging_fraction': 0.9993807771263824,\n",
    "#  'verbose': -1,\n",
    "#  'force_col_wise': True,\n",
    "#  'feature_pre_filter': False,\n",
    "#  'lambda_l1': 8.200000000000001,\n",
    "#  'lambda_l2': 103,\n",
    "#  'min_child_samples': 20}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 767,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimize hyperparameters of lgb\n",
    "# train_data = lgb.Dataset(X_train, label=y_train)\n",
    "# valid_data = lgb.Dataset(X_test, label=y_test)\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "# predict\n",
    "\n",
    "# optimize hyperparameters of lgb\n",
    "\n",
    "# def objective_lgb(trial):\n",
    "\n",
    "#     boosting_types = [\"gbdt\", \"rf\", \"dart\"]\n",
    "#     boosting_type = trial.suggest_categorical(\"boosting_type\", boosting_types)\n",
    "\n",
    "#     params = {\n",
    "#         'objective': 'binary',\n",
    "#         'boosting_type': trial.suggest_categorical(\"boosting_type\", [\"gbdt\", \"rf\", \"dart\"]),\n",
    "#         'max_depth': trial.suggest_int('max_depth', 2, 100),\n",
    "#         'min_child_samples': trial.suggest_int('min_child_samples', 0, 1000),\n",
    "#         'metric': 'mae',\n",
    "#         'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.5),\n",
    "#         'feature_fraction': trial.suggest_float('feature_fraction', 0.2, 0.95, step=0.1),\n",
    "#         'num_leaves': trial.suggest_int('num_leaves', 20, 3000, step=20),\n",
    "#         'bagging_freq': trial.suggest_categorical('bagging_freq', [1]),\n",
    "#         'bagging_fraction': trial.suggest_float('bagging_fraction', 0.2, 0.95, step=0.1),\n",
    "#         'reg_alpha': trial.suggest_float(\"reg_alpha\", 0, 100, step=0.1),\n",
    "#         'reg_lambda': trial.suggest_int(\"reg_lambda\", 0, 1000, step=1),\n",
    "#         'verbosity': -1,\n",
    "#     }\n",
    "\n",
    "#     lgbm = LGBMRegressor(**params)\n",
    "#     lgbm.fit(X_lgb_train_pre, y_lgb_train_pre, eval_set=[(X_lgb_test_pre, y_lgb_test_pre)],early_stopping_rounds=100, verbose=False)\n",
    "#     pred_lgb=lgbm.predict(X_lgb_test_pre)\n",
    "#     mae = mean_absolute_error(y_lgb_test_pre, pred_lgb)\n",
    "#     return mae\n",
    "\n",
    "\n",
    "# study = optuna.create_study(\n",
    "#     direction=\"minimize\",\n",
    "#     sampler=TPESampler(seed=42),\n",
    "#     pruner=MedianPruner(n_warmup_steps=50),\n",
    "# ) \n",
    "\n",
    "# study.optimize(objective_lgb, n_trials=100, show_progress_bar=True)\n",
    "\n",
    "# study.best_trial.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'boosting_type': 'dart',\n",
    " 'max_depth': 10,\n",
    " 'learning_rate': 0.2759844445088989,\n",
    " 'feature_fraction': 0.8,\n",
    " 'num_leaves': 360,\n",
    " 'bagging_freq': 1,\n",
    " 'bagging_fraction': 0.8,\n",
    " 'reg_alpha': 8.200000000000001,\n",
    " 'reg_lambda': 103}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 768,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network\n",
    "\n",
    "import kerastuner as kt\n",
    "\n",
    "# class ClearTrainingOutput(tf.keras.callbacks.Callback):\n",
    "#     def on_train_end(*args, **kwargs):\n",
    "#         print(\"训练完成，调用回调方法\")\n",
    "\n",
    "# def model_builder(hp):\n",
    "#     model = Sequential()\n",
    "#     # Tune the number of units in the first Dense layer\n",
    "#     # Choose an optimal value between 32-512\n",
    "#     hp_units = hp.Int('units', min_value=32, max_value=512, step=32)\n",
    "#     model.add(Dense(units=hp_units, activation='relu'))\n",
    "#     model.add(Dense(units=hp_units, activation='relu'))\n",
    "#     model.add(Dense(units=1, activation='relu'))\n",
    "\n",
    "#     # Tune the learning rate for the optimizer\n",
    "#     # Choose an optimal value from 0.01, 0.001, or 0.0001\n",
    "#     hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "\n",
    "#     model.compile(optimizer=Adam(learning_rate=hp_learning_rate),\n",
    "#                   loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "#                   metrics=['accuracy'])  # accuracy，用于判断模型效果的函数\n",
    "#     return model\n",
    "    \n",
    "# tuner = kt.Hyperband(model_builder,\n",
    "#                     objective='val_accuracy',  # 优化的目标，验证集accuracy\n",
    "#                     max_epochs=10,  # 最大迭代次数\n",
    "#                     factor=3)\n",
    "\n",
    "# tuner.search(X_train, y_train, epochs=10, validation_data=(X_test, y_test),\n",
    "#                  callbacks=[ClearTrainingOutput()])\n",
    "\n",
    "# tuner.get_best_hyperparameters(num_trials=1)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 769,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboard.plugins.hparams import api as hp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 770,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # hyperparameter tuning for neural network\n",
    "\n",
    "# HP_NUM_UNITS1 = hp.HParam('num_units1', hp.Discrete([16, 32, 64, 128]))\n",
    "# HP_NUM_UNITS2 = hp.HParam('num_units2', hp.Discrete([16, 32, 64, 128]))\n",
    "# HP_NUM_UNITS3 = hp.HParam('num_units3', hp.Discrete([16, 32, 64, 128]))\n",
    "# HP_DROPOUT = hp.HParam('dropout', hp.RealInterval(0.1, 0.2))\n",
    "\n",
    "# METRIC_ACCURACY = 'accuracy'\n",
    "\n",
    "# with tf.summary.create_file_writer('logs/hparam_tuning').as_default():\n",
    "#   hp.hparams_config(\n",
    "#     hparams=[HP_NUM_UNITS1, HP_NUM_UNITS2, HP_NUM_UNITS3],\n",
    "#     metrics=[hp.Metric(METRIC_ACCURACY, display_name='Accuracy')],\n",
    "#   )\n",
    "\n",
    "# def train_test_model(hparams):\n",
    "#   model = tf.keras.models.Sequential([\n",
    "#     tf.keras.layers.Dense(hparams[HP_NUM_UNITS1], activation=tf.nn.relu),\n",
    "#     tf.keras.layers.Dense(hparams[HP_NUM_UNITS2], activation=tf.nn.relu),\n",
    "#     tf.keras.layers.Dense(hparams[HP_NUM_UNITS3], activation=tf.nn.relu),\n",
    "#     tf.keras.layers.Dense(1, activation='sigmoid'),\n",
    "#   ])\n",
    "#   model.compile(\n",
    "#       optimizer='adam',\n",
    "#       loss='BinaryCrossentropy',\n",
    "#       metrics=['accuracy'],\n",
    "#   )\n",
    "\n",
    "#   model.fit(X_kbest_train, y_kbest_train, epochs=5, batch_size=512) # Run with 1 epoch to speed things up for demo purposes\n",
    "#   _, accuracy = model.evaluate(X_kbest_test, y_kbest_test)\n",
    "#   return accuracy\n",
    "\n",
    "# def run(run_dir, hparams):\n",
    "#   with tf.summary.create_file_writer(run_dir).as_default():\n",
    "#     hp.hparams(hparams)  # record the values used in this trial\n",
    "#     accuracy = train_test_model(hparams)\n",
    "#     tf.summary.scalar(METRIC_ACCURACY, accuracy, step=1)\n",
    "\n",
    "# session_num = 0\n",
    "\n",
    "# for num_units1 in HP_NUM_UNITS1.domain.values:\n",
    "#     for num_units2 in HP_NUM_UNITS2.domain.values:\n",
    "#       for num_units3 in HP_NUM_UNITS3.domain.values:\n",
    "#             hparams = {\n",
    "#                 HP_NUM_UNITS1: num_units1,\n",
    "#                 HP_NUM_UNITS2: num_units2,\n",
    "#                 HP_NUM_UNITS3: num_units3\n",
    "#             }\n",
    "#             run_name = \"run-%d\" % session_num\n",
    "#             print('--- Starting trial: %s' % run_name)\n",
    "#             print({h.name: hparams[h] for h in hparams})\n",
    "#             run('logs/hparam_tuning/' + run_name, hparams)\n",
    "#             session_num += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 772,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- TRAINING ---------\n",
      "Epoch 1/10\n",
      "254/254 [==============================] - 1s 4ms/step - loss: 0.2839 - Accuracy: 0.9135 - val_loss: 0.2279 - val_Accuracy: 0.9222\n",
      "Epoch 2/10\n",
      "254/254 [==============================] - 1s 3ms/step - loss: 0.2262 - Accuracy: 0.9219 - val_loss: 0.2238 - val_Accuracy: 0.9226\n",
      "Epoch 3/10\n",
      "254/254 [==============================] - 1s 3ms/step - loss: 0.2226 - Accuracy: 0.9228 - val_loss: 0.2227 - val_Accuracy: 0.9225\n",
      "Epoch 4/10\n",
      "254/254 [==============================] - 1s 4ms/step - loss: 0.2213 - Accuracy: 0.9231 - val_loss: 0.2208 - val_Accuracy: 0.9229\n",
      "Epoch 5/10\n",
      "254/254 [==============================] - 1s 3ms/step - loss: 0.2206 - Accuracy: 0.9234 - val_loss: 0.2195 - val_Accuracy: 0.9226\n",
      "Epoch 6/10\n",
      "254/254 [==============================] - 1s 3ms/step - loss: 0.2202 - Accuracy: 0.9234 - val_loss: 0.2194 - val_Accuracy: 0.9230\n",
      "Epoch 7/10\n",
      "254/254 [==============================] - 1s 3ms/step - loss: 0.2193 - Accuracy: 0.9235 - val_loss: 0.2216 - val_Accuracy: 0.9231\n",
      "Epoch 8/10\n",
      "254/254 [==============================] - 1s 3ms/step - loss: 0.2188 - Accuracy: 0.9235 - val_loss: 0.2199 - val_Accuracy: 0.9231\n",
      "Epoch 9/10\n",
      "254/254 [==============================] - 1s 3ms/step - loss: 0.2186 - Accuracy: 0.9236 - val_loss: 0.2207 - val_Accuracy: 0.9232\n",
      "Epoch 10/10\n",
      "254/254 [==============================] - 1s 3ms/step - loss: 0.2180 - Accuracy: 0.9239 - val_loss: 0.2207 - val_Accuracy: 0.9222\n",
      "1016/1016 - 1s - loss: 0.2207 - Accuracy: 0.9222 - 635ms/epoch - 625us/step\n",
      "1016/1016 [==============================] - 1s 576us/step\n",
      "5021/5021 [==============================] - 2s 476us/step\n",
      "([0.22068017721176147, 0.9221845865249634], array([[9.7067487e-01],\n",
      "       [9.5303768e-01],\n",
      "       [9.0532500e-01],\n",
      "       ...,\n",
      "       [3.1648385e-03],\n",
      "       [1.3174102e-04],\n",
      "       [9.6206820e-01]], dtype=float32), 0.22068000747883024)\n"
     ]
    }
   ],
   "source": [
    "def optimized_nn(X_train1, y_train1, X_test1, y_test1, input_valid_kbest):\n",
    "    model = Sequential([\n",
    "        Dense(128,activation='relu',name='input_layer'),\n",
    "        Dense(16,activation='relu',name='hidden_layer1'),\n",
    "        Dense(32,activation='relu',name='hidden_layer2'),\n",
    "        Dense(1, activation='sigmoid', name='output')])\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam()\n",
    "   \n",
    "    model.compile(optimizer=optimizer,\n",
    "                loss='BinaryCrossentropy',\n",
    "                metrics='Accuracy')\n",
    "\n",
    "    print('--------- TRAINING ---------')\n",
    "    history = model.fit(x=X_train1, y=y_train1, validation_data=(X_test1, y_test1), epochs = 10, batch_size=512)  \n",
    "    score = model.evaluate(X_test1,  y_test1, verbose=2)\n",
    "    y_pred = model.predict(X_test1)\n",
    "    # y_pred = sc_y.inverse_transform(y_pred)\n",
    "    y_estimate = model.predict(input_valid_kbest)\n",
    "    logloss = log_loss(y_test1, y_pred)\n",
    "\n",
    "    return score, y_estimate, logloss\n",
    "\n",
    "pred_nn = optimized_nn(X_kbest_train_pre, y_kbest_train_pre, X_kbest_test_pre, y_kbest_test_pre, input_valid_kbest)\n",
    "print(pred_nn)\n",
    "solution_nn = pd.DataFrame(data=pred_nn[1], columns=['preds'])\n",
    "solution_nn.to_csv('solutions/Classification_HauLamFong_neuralnetwork.txt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 773,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import RandomizedSearchCV\n",
    "# from xgboost import XGBRegressor\n",
    "# from scipy.stats import randint, poisson\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "# from bayes_opt import BayesianOptimization\n",
    "\n",
    "# # hyperparameters tuning for xgboost\n",
    "\n",
    "\n",
    "# def xgboost_CrossValidation(max_depth, min_child_weight, n_estimators, gamma, subsample, colsample_bytree, booster, objective, learning_rate, eval_metric, data, targets):\n",
    "#     \"\"\"Decision Tree cross validation.\n",
    "#        Fits a Decision Tree with the given paramaters to the target \n",
    "#        given data, calculated a CV accuracy score and returns the mean.\n",
    "#        The goal is to find combinations of max_depth, min_samples_leaf \n",
    "#        that maximize the accuracy\n",
    "#     \"\"\"\n",
    "    \n",
    "#     estimator = XGBRegressor(random_state=42, \n",
    "#                                        max_depth=max_depth, \n",
    "#                                        min_child_weight=min_child_weight, n_estimators=n_estimators, gamma=gamma, subsample=subsample, colsample_bytree=colsample_bytree, booster=booster, objective=objective, learning_rate=learning_rate, eval_metric=eval_metric)\n",
    "    \n",
    "#     cval = cross_val_score(estimator, data, targets, scoring='accuracy', cv=5)\n",
    "    \n",
    "#     return cval.mean()\n",
    "\n",
    "\n",
    "# def optimize_xgboost(data, targets, pars, n_iter=5):\n",
    "#     \"\"\"Apply Bayesian Optimization to Decision Tree parameters.\"\"\"\n",
    "    \n",
    "#     def crossval_wrapper(max_depth, min_child_weight, n_estimators, gamma, subsample, colsample_bytree, booster, objective, learning_rate, eval_metric):\n",
    "#         \"\"\"Wrapper of Decision Tree cross validation. \n",
    "#            Notice how we ensure max_depth, min_samples_leaf \n",
    "#            are casted to integer before we pass them along.\n",
    "#         \"\"\"\n",
    "#         return xgboost_CrossValidation(max_depth=int(max_depth), \n",
    "#                                             min_child_weight=int(min_child_weight), n_estimators=n_estimators, gamma=gamma, subsample=subsample, colsample_bytree=colsample_bytree, booster=booster, objective=objective, learning_rate=learning_rate, eval_metric=eval_metric,\n",
    "#                                             data=data, \n",
    "#                                             targets=targets)\n",
    "\n",
    "#     optimizer = BayesianOptimization(f=crossval_wrapper, \n",
    "#                                      pbounds=pars, \n",
    "#                                      random_state=42, \n",
    "#                                      verbose=2)\n",
    "#     optimizer.maximize(init_points=4, n_iter=n_iter)\n",
    "\n",
    "#     return optimizer\n",
    "\n",
    "# params = {\n",
    "#     'n_estimators':(0,1000),\n",
    "#     'min_child_weight':(0, 1000), \n",
    "#     'gamma':(0, 6),  \n",
    "#     'subsample':(1, 11),\n",
    "#     'colsample_bytree':(1, 11), \n",
    "#     'max_depth': (3, 100),\n",
    "#     'objective': ['binary:logistic', 'binary:logitraw'],\n",
    "#     'booster': ['gbtree', 'gblinear', 'dart'],\n",
    "#     'eval_metric': ['logloss'],\n",
    "#     'learning_rate': (0.01, 1),\n",
    "# }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 774,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# BayesianOptimization = optimize_xgboost(X_shap_train_pre, \n",
    "#                                              y_shap_train_pre, \n",
    "#                                              params, \n",
    "#                                              n_iter=5)\n",
    "# print(BayesianOptimization.max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 775,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# def objective_xgboost(trial):\n",
    "    \n",
    "#     params = {\n",
    "#     'n_estimators':trial.suggest_int('n_estimators', 0, 100),\n",
    "#     'verbosity': 0,\n",
    "#     'reg_alpha':trial.suggest_int('reg_alpha', 0, 5),\n",
    "#     'reg_lambda':trial.suggest_int('reg_lambda', 0, 5),\n",
    "#     'min_child_weight':trial.suggest_int('min_child_weight', 0, 100), \n",
    "#     'gamma':trial.suggest_int('gamma', 0, 6),  \n",
    "#     'subsample':trial.suggest_discrete_uniform('subsample', 0.1, 1, 0.01),\n",
    "#     'colsample_bytree':trial.suggest_discrete_uniform('colsample_bytree', 0.1, 1, 0.01), \n",
    "#     'max_depth': trial.suggest_int('max_depth', 3, 100),\n",
    "#     'objective': trial.suggest_categorical('objective', ['binary:logistic', 'binary:logitraw']),\n",
    "#     'booster': trial.suggest_categorical('booster',['gbtree', 'gblinear', 'dart']),\n",
    "#     'eval_metric': ['logloss'],\n",
    "#     'learning_rate': trial.suggest_loguniform('learning_rate', 0.001, 1),\n",
    "# }\n",
    "\n",
    "#     model = xgboost.XGBRegressor(**params)\n",
    "#     model.fit(X_shap_train_pre,y_shap_train_pre, eval_set=[(X_shap_test_pre, y_shap_test_pre)], early_stopping_rounds=50, verbose=False)\n",
    "#     preds = model.predict(X_shap_test_pre)\n",
    "#     rmse = mean_squared_error(y_shap_test_pre, preds, squared=False)\n",
    "\n",
    "#     return rmse\n",
    "\n",
    "# ntrial = 50\n",
    "# study = optuna.create_study(direction='minimize', sampler=TPESampler(seed=42), pruner=MedianPruner(n_warmup_steps=50))\n",
    "# study.optimize(objective_xgboost, n_trials=ntrial, show_progress_bar=True)\n",
    "\n",
    "# print('Best trial: score {},\\nparams {}'.format(study.best_trial.value,study.best_trial.params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 776,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0.9638293 , 0.8908489 , 0.9439875 , ..., 0.05447009, 0.01041053,\n",
      "       0.9275919 ], dtype=float32), 0.9424615384615385, 0.1453568831990903, 119811)\n"
     ]
    }
   ],
   "source": [
    "def optimized_xgboost(X_train1, y_train1, X_test1, y_test1, input_valid_shap):\n",
    "\n",
    "    params = {'n_estimators': 80, 'reg_alpha': 4, 'reg_lambda': 2, 'min_child_weight': 7, 'gamma': 0, 'subsample': 0.86,\n",
    "     'colsample_bytree': 0.76, 'max_depth': 87, 'objective': 'binary:logistic', 'booster': 'dart', 'learning_rate': 0.08192147965761523}\n",
    "    model = xgboost.XGBRegressor(**params)\n",
    "    model.fit(X_train1, y_train1)\n",
    "    y_pred_xgboost = model.predict(X_test1)\n",
    "    y_estimate = model.predict(input_valid_shap)\n",
    "    acc = accuracy_score(np.around(y_pred_xgboost), y_test1)\n",
    "    logloss = log_loss(y_test1, y_pred_xgboost)\n",
    "    n_electrons = np.sum(np.round(y_estimate)==1)\n",
    "\n",
    "    return y_estimate, acc, logloss, n_electrons\n",
    "\n",
    "pred_xgboost = optimized_xgboost(X_shap_train_pre, y_shap_train_pre, X_shap_test_pre, y_shap_test_pre, input_valid_shap)\n",
    "print(pred_xgboost)\n",
    "solution_xg = pd.DataFrame(data=pred_xgboost[0], columns=['preds'])\n",
    "solution_xg.to_csv('solutions/Classification_HauLamFong_xgboost.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 777,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 97199, number of negative: 32801\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007189 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[LightGBM] [Info] Total Bins 4136\n",
      "[LightGBM] [Info] Number of data points in the train set: 130000, number of used features: 20\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.747685 -> initscore=1.086301\n",
      "[LightGBM] [Info] Start training from score 1.086301\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1]\tvalid_0's binary_logloss: 0.349103\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2]\tvalid_0's binary_logloss: 0.274134\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3]\tvalid_0's binary_logloss: 0.230989\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4]\tvalid_0's binary_logloss: 0.204308\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[5]\tvalid_0's binary_logloss: 0.18678\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[6]\tvalid_0's binary_logloss: 0.175738\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[7]\tvalid_0's binary_logloss: 0.167835\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[8]\tvalid_0's binary_logloss: 0.162197\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[9]\tvalid_0's binary_logloss: 0.158373\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[10]\tvalid_0's binary_logloss: 0.155465\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[11]\tvalid_0's binary_logloss: 0.15351\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[12]\tvalid_0's binary_logloss: 0.151855\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[13]\tvalid_0's binary_logloss: 0.150858\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[14]\tvalid_0's binary_logloss: 0.150316\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[15]\tvalid_0's binary_logloss: 0.149929\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[16]\tvalid_0's binary_logloss: 0.149129\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[17]\tvalid_0's binary_logloss: 0.14876\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[18]\tvalid_0's binary_logloss: 0.148329\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[19]\tvalid_0's binary_logloss: 0.148389\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[20]\tvalid_0's binary_logloss: 0.148303\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[21]\tvalid_0's binary_logloss: 0.148089\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[22]\tvalid_0's binary_logloss: 0.148114\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[23]\tvalid_0's binary_logloss: 0.148094\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[24]\tvalid_0's binary_logloss: 0.147853\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[25]\tvalid_0's binary_logloss: 0.147935\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[26]\tvalid_0's binary_logloss: 0.147786\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[27]\tvalid_0's binary_logloss: 0.147942\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[28]\tvalid_0's binary_logloss: 0.147815\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[29]\tvalid_0's binary_logloss: 0.148022\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[30]\tvalid_0's binary_logloss: 0.148279\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[31]\tvalid_0's binary_logloss: 0.148601\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[32]\tvalid_0's binary_logloss: 0.148851\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[33]\tvalid_0's binary_logloss: 0.149015\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[34]\tvalid_0's binary_logloss: 0.149116\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[35]\tvalid_0's binary_logloss: 0.149109\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[36]\tvalid_0's binary_logloss: 0.149291\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[37]\tvalid_0's binary_logloss: 0.149502\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[38]\tvalid_0's binary_logloss: 0.149716\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[39]\tvalid_0's binary_logloss: 0.149956\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[40]\tvalid_0's binary_logloss: 0.149983\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[41]\tvalid_0's binary_logloss: 0.150176\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[42]\tvalid_0's binary_logloss: 0.15031\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[43]\tvalid_0's binary_logloss: 0.150496\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[44]\tvalid_0's binary_logloss: 0.150851\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[45]\tvalid_0's binary_logloss: 0.15093\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[46]\tvalid_0's binary_logloss: 0.151104\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[47]\tvalid_0's binary_logloss: 0.151333\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[48]\tvalid_0's binary_logloss: 0.151781\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[49]\tvalid_0's binary_logloss: 0.152122\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[50]\tvalid_0's binary_logloss: 0.152398\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[51]\tvalid_0's binary_logloss: 0.15293\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[52]\tvalid_0's binary_logloss: 0.152944\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[53]\tvalid_0's binary_logloss: 0.152996\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[54]\tvalid_0's binary_logloss: 0.153127\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[55]\tvalid_0's binary_logloss: 0.153276\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[56]\tvalid_0's binary_logloss: 0.153255\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[57]\tvalid_0's binary_logloss: 0.153589\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[58]\tvalid_0's binary_logloss: 0.153684\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[59]\tvalid_0's binary_logloss: 0.154077\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[60]\tvalid_0's binary_logloss: 0.154244\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[61]\tvalid_0's binary_logloss: 0.154409\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[62]\tvalid_0's binary_logloss: 0.154685\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[63]\tvalid_0's binary_logloss: 0.154761\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[64]\tvalid_0's binary_logloss: 0.154904\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[65]\tvalid_0's binary_logloss: 0.155204\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[66]\tvalid_0's binary_logloss: 0.155419\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[67]\tvalid_0's binary_logloss: 0.15555\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[68]\tvalid_0's binary_logloss: 0.15581\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[69]\tvalid_0's binary_logloss: 0.156087\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[70]\tvalid_0's binary_logloss: 0.156253\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[71]\tvalid_0's binary_logloss: 0.156548\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[72]\tvalid_0's binary_logloss: 0.156644\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[73]\tvalid_0's binary_logloss: 0.156872\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[74]\tvalid_0's binary_logloss: 0.157113\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[75]\tvalid_0's binary_logloss: 0.157175\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[76]\tvalid_0's binary_logloss: 0.157346\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[77]\tvalid_0's binary_logloss: 0.157778\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[78]\tvalid_0's binary_logloss: 0.157999\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[79]\tvalid_0's binary_logloss: 0.157842\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[80]\tvalid_0's binary_logloss: 0.157943\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[81]\tvalid_0's binary_logloss: 0.158001\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[82]\tvalid_0's binary_logloss: 0.158482\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[83]\tvalid_0's binary_logloss: 0.158524\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[84]\tvalid_0's binary_logloss: 0.158595\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[85]\tvalid_0's binary_logloss: 0.158685\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[86]\tvalid_0's binary_logloss: 0.158832\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[87]\tvalid_0's binary_logloss: 0.158938\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[88]\tvalid_0's binary_logloss: 0.159054\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[89]\tvalid_0's binary_logloss: 0.159229\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[90]\tvalid_0's binary_logloss: 0.159372\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[91]\tvalid_0's binary_logloss: 0.159511\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[92]\tvalid_0's binary_logloss: 0.15966\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[93]\tvalid_0's binary_logloss: 0.159664\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[94]\tvalid_0's binary_logloss: 0.159831\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[95]\tvalid_0's binary_logloss: 0.15989\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[96]\tvalid_0's binary_logloss: 0.159917\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[97]\tvalid_0's binary_logloss: 0.16007\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[98]\tvalid_0's binary_logloss: 0.160291\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[99]\tvalid_0's binary_logloss: 0.160377\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[100]\tvalid_0's binary_logloss: 0.16058\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[101]\tvalid_0's binary_logloss: 0.160747\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[102]\tvalid_0's binary_logloss: 0.160511\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[103]\tvalid_0's binary_logloss: 0.160703\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[104]\tvalid_0's binary_logloss: 0.160847\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[105]\tvalid_0's binary_logloss: 0.161131\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[106]\tvalid_0's binary_logloss: 0.161257\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[107]\tvalid_0's binary_logloss: 0.16124\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[108]\tvalid_0's binary_logloss: 0.161503\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[109]\tvalid_0's binary_logloss: 0.161669\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[110]\tvalid_0's binary_logloss: 0.161669\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[111]\tvalid_0's binary_logloss: 0.161827\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[112]\tvalid_0's binary_logloss: 0.162051\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[113]\tvalid_0's binary_logloss: 0.162215\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[114]\tvalid_0's binary_logloss: 0.162383\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[115]\tvalid_0's binary_logloss: 0.16242\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[116]\tvalid_0's binary_logloss: 0.162492\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[117]\tvalid_0's binary_logloss: 0.162806\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[118]\tvalid_0's binary_logloss: 0.162894\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[119]\tvalid_0's binary_logloss: 0.16278\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[120]\tvalid_0's binary_logloss: 0.162939\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[121]\tvalid_0's binary_logloss: 0.163017\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[122]\tvalid_0's binary_logloss: 0.162982\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[123]\tvalid_0's binary_logloss: 0.163041\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[124]\tvalid_0's binary_logloss: 0.163048\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[125]\tvalid_0's binary_logloss: 0.163114\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[126]\tvalid_0's binary_logloss: 0.16337\n",
      "Early stopping, best iteration is:\n",
      "[26]\tvalid_0's binary_logloss: 0.147786\n",
      "(array([0.92176033, 0.88397992, 0.91612724, ..., 0.0543168 , 0.00469723,\n",
      "       0.60956197]), 0.14778584808488007, 0.9414769230769231)\n"
     ]
    }
   ],
   "source": [
    "def optimized_lgb(X_train1, y_train1, X_test1, y_test1, input_valid_lgb):\n",
    "\n",
    "    params = {'boosting_type': 'gbdt',\n",
    "                'max_depth': 19,\n",
    "                'min_child_samples': 139,\n",
    "                'learning_rate': 0.3851342929999186,\n",
    "                'feature_fraction': 0.8,\n",
    "                'num_leaves': 1020,\n",
    "                'bagging_freq': 1,\n",
    "                'bagging_fraction': 0.8,\n",
    "                'reg_alpha': 3.3000000000000003,\n",
    "                'reg_lambda': 1,\n",
    "                'objective':'binary'\n",
    "            }\n",
    "\n",
    "    lgb_train = lgb.Dataset(X_train1, y_train1)\n",
    "    lgb_test = lgb.Dataset(X_test1, y_test1)\n",
    "    lgbm = lgb.train(params, lgb_train,num_boost_round=1000, valid_sets=lgb_test, early_stopping_rounds=100)\n",
    "    pred_lgb=lgbm.predict(X_test1)\n",
    "    logloss = log_loss(y_test1, pred_lgb)\n",
    "    acc = accuracy_score(y_test1, np.around(pred_lgb))\n",
    "    y_estimate = lgbm.predict(input_valid_lgb)\n",
    "\n",
    "    \n",
    "    return y_estimate, logloss, acc\n",
    "\n",
    "pred_lgb = optimized_lgb(X_lgb_train_pre, y_lgb_train_pre, X_lgb_test_pre, y_lgb_test_pre, input_valid_lgb)\n",
    "print(pred_lgb)\n",
    "\n",
    "solution_lgb = pd.DataFrame(data=pred_lgb[0], columns=['preds'])\n",
    "solution_lgb.to_csv('solutions/Classification_HauLamFong_lightgbm.txt')\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4e3119b6b440005e83014445b502bc062a01c9850f9c4ea1b0d68db6d948f423"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
