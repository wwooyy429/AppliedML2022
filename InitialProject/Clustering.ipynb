{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import pandas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import tree\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.metrics import accuracy_score, r2_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "import shap\n",
    "import lightgbm as lgb \n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from optuna.integration import LightGBMPruningCallback\n",
    "from optuna.pruners import MedianPruner\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.metrics import mean_absolute_error \n",
    "from sklearn.feature_selection import SelectKBest, f_regression, chi2, f_classif, mutual_info_regression\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import mean_absolute_error, silhouette_score\n",
    "from sklearn import cluster, datasets, mixture\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from itertools import cycle, islice\n",
    "from sklearn.mixture import BayesianGaussianMixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training data set: (162500, 166)\n",
      "Shape of test data set: (160651, 164)\n"
     ]
    }
   ],
   "source": [
    "# read data\n",
    "def load_data(name):\n",
    "    with h5py.File(f'{name}.h5', 'r') as f:\n",
    "        filename = name.split('/')[-1]\n",
    "        return pandas.DataFrame(f[filename][:], dtype=np.float64)\n",
    "\n",
    "train = load_data('train')\n",
    "test  = load_data('test')\n",
    "\n",
    "print (f'Shape of training data set: {train.shape}')\n",
    "print (f'Shape of test data set: {test.shape}')\n",
    "\n",
    "all_variables = ['actualInteractionsPerCrossing', 'averageInteractionsPerCrossing', 'correctedActualMu', 'correctedAverageMu', 'correctedScaledActualMu', 'correctedScaledAverageMu', 'NvtxReco', 'p_nTracks', 'p_pt_track', 'p_eta', 'p_phi', 'p_charge', 'p_qOverP', 'p_z0', 'p_d0', 'p_sigmad0', 'p_d0Sig', 'p_EptRatio', 'p_dPOverP', 'p_z0theta', 'p_etaCluster', 'p_phiCluster', 'p_eCluster', 'p_rawEtaCluster', 'p_rawPhiCluster', 'p_rawECluster', 'p_eClusterLr0', 'p_eClusterLr1', 'p_eClusterLr2', 'p_eClusterLr3', 'p_etaClusterLr1', 'p_etaClusterLr2', 'p_phiClusterLr2', 'p_eAccCluster', 'p_f0Cluster', 'p_etaCalo', 'p_phiCalo', 'p_eTileGap3Cluster', 'p_cellIndexCluster', 'p_phiModCalo', 'p_etaModCalo', 'p_dPhiTH3', 'p_R12', 'p_fTG3', 'p_weta2', 'p_Reta', 'p_Rphi', 'p_Eratio', 'p_f1', 'p_f3', 'p_Rhad', 'p_Rhad1', 'p_deltaEta1', 'p_deltaPhiRescaled2', 'p_TRTPID', 'p_TRTTrackOccupancy', 'p_numberOfInnermostPixelHits', 'p_numberOfPixelHits', 'p_numberOfSCTHits', 'p_numberOfTRTHits', 'p_numberOfTRTXenonHits', 'p_chi2', 'p_ndof', 'p_SharedMuonTrack', 'p_E7x7_Lr2', 'p_E7x7_Lr3', 'p_E_Lr0_HiG', 'p_E_Lr0_LowG', 'p_E_Lr0_MedG', 'p_E_Lr1_HiG', 'p_E_Lr1_LowG', 'p_E_Lr1_MedG', 'p_E_Lr2_HiG', 'p_E_Lr2_LowG', 'p_E_Lr2_MedG', 'p_E_Lr3_HiG', 'p_E_Lr3_LowG', 'p_E_Lr3_MedG', 'p_ambiguityType', 'p_asy1', 'p_author', 'p_barys1', 'p_core57cellsEnergyCorrection', 'p_deltaEta0', 'p_deltaEta2', 'p_deltaEta3', 'p_deltaPhi0', 'p_deltaPhi1', 'p_deltaPhi2', 'p_deltaPhi3', 'p_deltaPhiFromLastMeasurement', 'p_deltaPhiRescaled0', 'p_deltaPhiRescaled1', 'p_deltaPhiRescaled3', 'p_e1152', 'p_e132', 'p_e235', 'p_e255', 'p_e2ts1', 'p_ecore', 'p_emins1', 'p_etconeCorrBitset', 'p_ethad', 'p_ethad1', 'p_f1core', 'p_f3core', 'p_maxEcell_energy', 'p_maxEcell_gain', 'p_maxEcell_time', 'p_maxEcell_x', 'p_maxEcell_y', 'p_maxEcell_z', 'p_nCells_Lr0_HiG', 'p_nCells_Lr0_LowG', 'p_nCells_Lr0_MedG', 'p_nCells_Lr1_HiG', 'p_nCells_Lr1_LowG', 'p_nCells_Lr1_MedG', 'p_nCells_Lr2_HiG', 'p_nCells_Lr2_LowG', 'p_nCells_Lr2_MedG', 'p_nCells_Lr3_HiG', 'p_nCells_Lr3_LowG', 'p_nCells_Lr3_MedG', 'p_pos', 'p_pos7', 'p_poscs1', 'p_poscs2', 'p_ptconeCorrBitset', 'p_ptconecoreTrackPtrCorrection', 'p_r33over37allcalo', 'p_topoetconeCorrBitset', 'p_topoetconecoreConeEnergyCorrection', 'p_topoetconecoreConeSCEnergyCorrection', 'p_weta1', 'p_widths1', 'p_widths2', 'p_wtots1', 'p_e233', 'p_e237', 'p_e277', 'p_e2tsts1', 'p_ehad1', 'p_emaxs1', 'p_fracs1', 'p_DeltaE', 'p_E3x5_Lr0', 'p_E3x5_Lr1', 'p_E3x5_Lr2', 'p_E3x5_Lr3', 'p_E5x7_Lr0', 'p_E5x7_Lr1', 'p_E5x7_Lr2', 'p_E5x7_Lr3', 'p_E7x11_Lr0', 'p_E7x11_Lr1', 'p_E7x11_Lr2', 'p_E7x11_Lr3', 'p_E7x7_Lr0', 'p_E7x7_Lr1' ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_variables = ['p_Rhad','p_Reta','p_deltaEta1', 'p_sigmad0',\n",
    "\t'p_Rphi',\t\n",
    "\t'p_ambiguityType',\t\n",
    "\t'p_ethad',\n",
    "\t'p_numberOfInnermostPixelHits']\n",
    "X_shap = test[shap_variables]\n",
    "sc_X_shap = preprocessing.StandardScaler()\n",
    "rb_X_shap = preprocessing.RobustScaler()\n",
    "X_shap = test[shap_variables]\n",
    "X_shap = sc_X_shap.fit_transform(X_shap)\n",
    "\n",
    "cluster_variables = ['p_Rhad','p_Reta','p_deltaEta1', 'p_sigmad0',\n",
    "\t'p_Rphi',\t\n",
    "\t'p_ambiguityType',\t\n",
    "\t'p_ethad',\n",
    "\t'p_numberOfInnermostPixelHits']\n",
    "\n",
    "norm_X_cluster = preprocessing.MinMaxScaler()\n",
    "sc_X_cluster = preprocessing.StandardScaler()\n",
    "X_cluster = test[cluster_variables]\n",
    "X_cluster = sc_X_cluster.fit_transform(X_cluster)\n",
    "\n",
    "\n",
    "kms_variables = ['p_deltaPhi0', 'p_deltaEta0', 'p_deltaPhiRescaled0','p_nCells_Lr0_HiG', 'p_E_Lr0_HiG', 'p_eClusterLr0', 'p_E3x5_Lr0','p_E7x11_Lr0']\n",
    "\n",
    "sc_X_kms = preprocessing.StandardScaler()\n",
    "rb_X_kms = preprocessing.RobustScaler()\n",
    "X_kms = test[kms_variables]\n",
    "X_kms = sc_X_kms.fit_transform(X_kms)\n",
    "\n",
    "variable_list_shap = pd.DataFrame(shap_variables, columns=['vars'])\n",
    "variable_list_shap.to_csv('solutions/Clustering_HauLamFong_kmeanshap_VariableList.txt')\n",
    "variable_list_kms = pd.DataFrame(kms_variables, columns=['vars'])\n",
    "variable_list_kms.to_csv('solutions/Clustering_HauLamFong_kmeanwcss_VariableList.txt')\n",
    "variable_list_gmm = pd.DataFrame(kms_variables, columns=['vars'])\n",
    "variable_list_gmm.to_csv('solutions/Clustering_HauLamFong_gmmwcss_VariableList.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "\n",
    "\n",
    "default_base = {\n",
    "    \"quantile\": 0.3,\n",
    "    \"eps\": 0.3,\n",
    "    \"damping\": 0.9,\n",
    "    \"preference\": -200,\n",
    "    \"n_neighbors\": 10,\n",
    "    \"n_clusters\": 3,\n",
    "    \"min_samples\": 20,\n",
    "    \"xi\": 0.05,\n",
    "    \"min_cluster_size\": 0.1,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# import warnings\n",
    "# from tqdm.notebook import tqdm, trange    # Progress bar.\n",
    "\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# from sklearn import cluster, datasets, mixture\n",
    "# from sklearn.neighbors import kneighbors_graph\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from itertools import cycle, islice\n",
    "\n",
    "# np.random.seed(0)\n",
    "\n",
    "# # ============\n",
    "# # Set up cluster parameters\n",
    "# # ============\n",
    "# plt.figure(figsize=(9 * 2 + 3, 13))\n",
    "# plt.subplots_adjust(\n",
    "#     left=0.02, right=0.98, bottom=0.001, top=0.95, wspace=0.05, hspace=0.01\n",
    "# )\n",
    "\n",
    "# plot_num = 1\n",
    "\n",
    "# default_base = {\n",
    "#     \"quantile\": 0.3,\n",
    "#     \"eps\": 0.3,\n",
    "#     \"damping\": 0.9,\n",
    "#     \"preference\": -200,\n",
    "#     \"n_neighbors\": 10,\n",
    "#     \"n_clusters\": 3,\n",
    "#     \"min_samples\": 20,\n",
    "#     \"xi\": 0.05,\n",
    "#     \"min_cluster_size\": 0.1,\n",
    "# }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # update parameters with dataset-specific values\n",
    "# params = default_base.copy()\n",
    "\n",
    "# # normalize dataset for easier parameter selection\n",
    "\n",
    "# # estimate bandwidth for mean shift\n",
    "# bandwidth = cluster.estimate_bandwidth(X_cluster, quantile=params[\"quantile\"])\n",
    "\n",
    "# # connectivity matrix for structured Ward\n",
    "# connectivity = kneighbors_graph(\n",
    "#     X_cluster, n_neighbors=params[\"n_neighbors\"], include_self=False\n",
    "# )\n",
    "# # make connectivity symmetric\n",
    "# connectivity = 0.5 * (connectivity + connectivity.T)\n",
    "\n",
    "# # ============\n",
    "# # Create cluster objects\n",
    "# # ============\n",
    "# ms = cluster.MeanShift(bandwidth=bandwidth, bin_seeding=True)\n",
    "# two_means = cluster.MiniBatchKMeans(n_clusters=params[\"n_clusters\"])\n",
    "# ward = cluster.AgglomerativeClustering(\n",
    "#     n_clusters=params[\"n_clusters\"], linkage=\"ward\", connectivity=connectivity\n",
    "# )\n",
    "# spectral = cluster.SpectralClustering(\n",
    "#     n_clusters=params[\"n_clusters\"],\n",
    "#     eigen_solver=\"arpack\",\n",
    "#     affinity=\"nearest_neighbors\",\n",
    "# )\n",
    "# dbscan = cluster.DBSCAN(eps=params[\"eps\"])\n",
    "# optics = cluster.OPTICS(\n",
    "#     min_samples=params[\"min_samples\"],\n",
    "#     xi=params[\"xi\"],\n",
    "#     min_cluster_size=params[\"min_cluster_size\"],\n",
    "# )\n",
    "# affinity_propagation = cluster.AffinityPropagation(\n",
    "#     damping=params[\"damping\"], preference=params[\"preference\"],\n",
    "# )\n",
    "# average_linkage = cluster.AgglomerativeClustering(\n",
    "#     linkage=\"average\",\n",
    "#     affinity=\"cityblock\",\n",
    "#     n_clusters=params[\"n_clusters\"],\n",
    "#     connectivity=connectivity,\n",
    "# )\n",
    "# birch = cluster.Birch(n_clusters=params[\"n_clusters\"])\n",
    "# gmm = mixture.GaussianMixture(\n",
    "#     n_components=params[\"n_clusters\"], covariance_type=\"full\"\n",
    "# )\n",
    "\n",
    "# clustering_algorithms = (\n",
    "#     (\"MiniBatch\\nKMeans\", two_means),\n",
    "#     (\"Affinity\\nPropagation\", affinity_propagation),\n",
    "#     (\"MeanShift\", ms),\n",
    "#     (\"Spectral\\nClustering\", spectral),\n",
    "#     (\"Ward\", ward),\n",
    "#     (\"Agglomerative\\nClustering\", average_linkage),\n",
    "#     (\"DBSCAN\", dbscan),\n",
    "#     (\"OPTICS\", optics),\n",
    "#     (\"BIRCH\", birch),\n",
    "#     (\"Gaussian\\nMixture\", gmm),\n",
    "# )\n",
    "\n",
    "# for name, algorithm in clustering_algorithms:\n",
    "#     t0 = time.time()\n",
    "\n",
    "#     # catch warnings related to kneighbors_graph\n",
    "#     with warnings.catch_warnings():\n",
    "#         warnings.filterwarnings(\n",
    "#             \"ignore\",\n",
    "#             message=\"the number of connected components of the \"\n",
    "#             + \"connectivity matrix is [0-9]{1,2}\"\n",
    "#             + \" > 1. Completing it to avoid stopping the tree early.\",\n",
    "#             category=UserWarning,\n",
    "#         )\n",
    "#         warnings.filterwarnings(\n",
    "#             \"ignore\",\n",
    "#             message=\"Graph is not fully connected, spectral embedding\"\n",
    "#             + \" may not work as expected.\",\n",
    "#             category=UserWarning,\n",
    "#         )\n",
    "#         algorithm.fit(X_cluster)\n",
    "\n",
    "#     t1 = time.time()\n",
    "#     if hasattr(algorithm, \"labels_\"):\n",
    "#         y_pred = algorithm.labels_.astype(int)\n",
    "#     else:\n",
    "#         y_pred = algorithm.predict(X_cluster)\n",
    "\n",
    "#     plt.subplot(1, len(clustering_algorithms), plot_num)\n",
    "    \n",
    "#     plt.title(name, size=18)\n",
    "\n",
    "\n",
    "#     colors = np.array(\n",
    "#         list(\n",
    "#             islice(\n",
    "#                 cycle(\n",
    "#                     [\n",
    "#                         \"#377eb8\",\n",
    "#                         \"#ff7f00\",\n",
    "#                         \"#4daf4a\",\n",
    "#                         \"#f781bf\",\n",
    "#                         \"#a65628\",\n",
    "#                         \"#984ea3\",\n",
    "#                         \"#999999\",\n",
    "#                         \"#e41a1c\",\n",
    "#                         \"#dede00\",\n",
    "#                     ]\n",
    "#                 ),\n",
    "#                 int(max(y_pred) + 1),\n",
    "#             )\n",
    "#         )\n",
    "#     )\n",
    "#     # add black color for outliers (if any)\n",
    "#     colors = np.append(colors, [\"#000000\"])\n",
    "#     plt.scatter(X_cluster[:, 0], X_cluster[:, 1], s=10, color=colors[y_pred])\n",
    "\n",
    "#     plt.xlim(-2.5, 2.5)\n",
    "#     plt.ylim(-2.5, 2.5)\n",
    "#     plt.xticks(())\n",
    "#     plt.yticks(())\n",
    "#     plt.text(\n",
    "#         0.99,\n",
    "#         0.01,\n",
    "#         (\"%.2fs\" % (t1 - t0)).lstrip(\"0\"),\n",
    "#         transform=plt.gca().transAxes,\n",
    "#         size=15,\n",
    "#         horizontalalignment=\"right\",\n",
    "#     )\n",
    "#     plot_num += 1\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_mean(ncluster, X):\n",
    "    k_means = cluster.KMeans(n_clusters=ncluster, init='k-means++')\n",
    "    k_means.fit(X)\n",
    "    y_pred_kms = k_means.predict(X)\n",
    "    score = 1\n",
    "    return y_pred_kms, score\n",
    "\n",
    "def gaussian(ncluster,X):\n",
    "    gmm = mixture.GaussianMixture(\n",
    "        n_components=ncluster, covariance_type='full')\n",
    "    gmm.fit(X)\n",
    "    y_pred_gmm = gmm.predict(X)\n",
    "    # score = silhouette_score(X, y_pred_gmm, metric='euclidean')\n",
    "    score = 1\n",
    "    return y_pred_gmm, score\n",
    "\n",
    "def birch(ncluster, X):\n",
    "    birch1 = cluster.Birch(n_clusters=ncluster)\n",
    "    birch1.fit(X)\n",
    "    y_pred_birch = birch1.predict(X_cluster)\n",
    "\n",
    "    return y_pred_birch\n",
    "\n",
    "# plt.title('Original data (3 groups)')\n",
    "# plt.scatter(X_cluster.T[0], X_cluster.T[3], c=y_pred)\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kmean: 105212\n",
      "kmean: 127253\n",
      "gmm: 98617\n",
      "kmean: 31722\n",
      "kmean: 16019\n",
      "gmm: 7442\n",
      "kmean: 23717\n",
      "kmean: 17379\n",
      "gmm: 54592\n",
      "0.12185024747310347\n",
      "0.062114496999440785\n",
      "0.1768952767275125\n",
      "160651.0\n",
      "160651.0\n",
      "160651.0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "ncluster = 3\n",
    "y_pred_kms1 = k_mean(ncluster, X_kms)\n",
    "y_pred_kms2 = k_mean(ncluster, X_shap)\n",
    "y_pred_gmm = gaussian(ncluster, X_kms)\n",
    "y_pred_gmm_kms = gaussian(ncluster, X_kms)\n",
    "sum_kmean1 = np.zeros(ncluster)\n",
    "sum_kmean2 = np.zeros(ncluster)\n",
    "sum_gmm = np.zeros(ncluster)\n",
    "\n",
    "\n",
    "# y_pred_birch = birch(ncluster, X_kms)\n",
    "for i in range(ncluster):\n",
    "    print(f'kmean: {np.sum(y_pred_kms1[0]==i)}')\n",
    "    sum_kmean1[i] = np.sum(y_pred_kms1[0] == i)\n",
    "    print(f'kmean: {np.sum(y_pred_kms2[0]==i)}')\n",
    "    sum_kmean2[i] = np.sum(y_pred_kms2[0] == i)\n",
    "    print(f'gmm: {np.sum(y_pred_gmm[0]==i)}')\n",
    "    sum_gmm[i] = np.sum(y_pred_gmm[0] == i)\n",
    "\n",
    "print(np.abs((119811-sum_kmean1.max())/119811))\n",
    "print(np.abs((119811-sum_kmean2.max())/119811))\n",
    "print(np.abs((119811-sum_gmm.max())/119811))\n",
    "\n",
    "print(np.sum(sum_kmean1))\n",
    "print(np.sum(sum_kmean2))\n",
    "print(np.sum(sum_gmm))\n",
    "\n",
    "print(y_pred_kms1[1])\n",
    "print(y_pred_kms2[1])\n",
    "print(y_pred_gmm[1])\n",
    "print(y_pred_gmm_kms[1])\n",
    "\n",
    "\n",
    "\n",
    "solution_kms1 = pd.DataFrame(data=y_pred_kms1[0], columns=['preds'])\n",
    "solution_kms1.to_csv('solutions/Clustering_HauLamFong_kmeanshap.txt')\n",
    "solution_kms2 = pd.DataFrame(data=y_pred_kms2[0], columns=['preds'])\n",
    "solution_kms2.to_csv('solutions/Clustering_HauLamFong_kmeanwcss.txt')\n",
    "solution_gmm = pd.DataFrame(data=y_pred_gmm[0], columns=['preds'])\n",
    "solution_gmm.to_csv('solutions/Clustering_HauLamFong_gmmwcss.txt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.804125833005802\n",
    "0.8486362845132446\n",
    "0.770468081116275"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum1 = 0\n",
    "# n = 9\n",
    "# # y_predict = k_mean(n)\n",
    "# accuracy = np.zeros(n)\n",
    "# sum1 = np.zeros(n)\n",
    "# nexp = 100\n",
    "# total = np.zeros([n,nexp])\n",
    "\n",
    "# for m in range(nexp):\n",
    "#     for i in range(2, n):\n",
    "#         pred = np.zeros(n)\n",
    "#         y_predict = k_mean(i)\n",
    "#         for j in range(n):\n",
    "#             pred[j] = np.sum(y_predict==j)\n",
    "        \n",
    "#         sum1[i] = np.max(pred)\n",
    "    \n",
    "#     total[:,m] = sum1\n",
    "\n",
    "\n",
    "# accuracy = np.abs((121495 - np.mean(total, axis=1)) / 121495)\n",
    "\n",
    "# # print(f'sum is :{sum1}')\n",
    "# print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 4096 or by setting the environment variable OMP_NUM_THREADS=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kmean: 8879\n",
      "kmean: 115023\n",
      "kmean: 9577\n",
      "kmean: 13926\n",
      "kmean: 13246\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import warnings\n",
    "from tqdm.notebook import tqdm, trange    # Progress bar.\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import cluster, datasets, mixture\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from itertools import cycle, islice\n",
    "\n",
    "params = {\n",
    "\"quantile\": 0.3,\n",
    "\"eps\": 0.3,\n",
    "\"damping\": 0.9,\n",
    "\"preference\": -200,\n",
    "\"n_neighbors\": 2,\n",
    "\"n_clusters\": 5,\n",
    "\"min_samples\": 20,\n",
    "\"xi\": 0.05,\n",
    "\"min_cluster_size\": 0.1,\n",
    "}\n",
    "ncluster = 5\n",
    "\n",
    "\n",
    "# estimate bandwidth for mean shift\n",
    "# bandwidth = cluster.estimate_bandwidth(X_cluster, quantile=params[\"quantile\"])\n",
    "\n",
    "# # connectivity matrix for structured Ward\n",
    "# connectivity = kneighbors_graph(\n",
    "#     X_cluster, n_neighbors=params[\"n_neighbors\"], include_self=False\n",
    "# )\n",
    "# # make connectivity symmetric\n",
    "# connectivity = 0.5 * (connectivity + connectivity.T)\n",
    "\n",
    "# ============\n",
    "# Create cluster objects\n",
    "# ============\n",
    "# ms = cluster.MeanShift(bin_seeding=True)\n",
    "# ms.fit(X_cluster)\n",
    "\n",
    "two_means = cluster.MiniBatchKMeans(n_clusters=params[\"n_clusters\"])\n",
    "two_means.fit(X_cluster)\n",
    "y_pred_kmean = two_means.predict(X_cluster)\n",
    "for i in range(ncluster):\n",
    "    print(f'kmean: {np.sum(y_pred_kmean==i)}')\n",
    "\n",
    "# ward = cluster.AgglomerativeClustering(\n",
    "#     n_clusters=params[\"n_clusters\"], linkage=\"ward\", connectivity=connectivity\n",
    "# )\n",
    "# ward.fit(X_cluster)\n",
    "\n",
    "# spectral = cluster.SpectralClustering(\n",
    "#     n_clusters=params[\"n_clusters\"],\n",
    "#     eigen_solver=\"arpack\",\n",
    "#     affinity=\"nearest_neighbors\",\n",
    "# )\n",
    "# spectral.fit(X_cluster)\n",
    "\n",
    "# dbscan = cluster.DBSCAN(algorithm='auto')\n",
    "# dbscan.fit(X_cluster)\n",
    "\n",
    "# optics = cluster.OPTICS(\n",
    "#     min_samples=params[\"min_samples\"],\n",
    "#     xi=params[\"xi\"],\n",
    "#     min_cluster_size=params[\"min_cluster_size\"],\n",
    "# )\n",
    "# optics.fit(X_cluster)\n",
    "\n",
    "# affinity_propagation = cluster.AffinityPropagation(\n",
    "#     damping=params[\"damping\"], preference=params[\"preference\"],\n",
    "# )\n",
    "# affinity_propagation.fit(X_cluster)\n",
    "\n",
    "# average_linkage = cluster.AgglomerativeClustering(\n",
    "#     linkage=\"average\",\n",
    "#     affinity=\"cityblock\",\n",
    "#     n_clusters=params[\"n_clusters\"], connectivity=connectivity\n",
    "# )\n",
    "# average_linkage.fit(X_cluster)\n",
    "\n",
    "# birch = cluster.Birch(n_clusters=params[\"n_clusters\"], threshold=0.7)\n",
    "# birch.fit(X_cluster)\n",
    "# y_pred_birch = birch.predict(X_cluster)\n",
    "# for i in range(ncluster):\n",
    "#     print(f'birch: {np.sum(y_pred_birch==i)}')\n",
    "\n",
    "def gaussian(ncluster,X):\n",
    "    gmm = mixture.GaussianMixture(\n",
    "        n_components=ncluster)\n",
    "    gmm.fit(X)\n",
    "    y_pred_gmm = gmm.predict(X)\n",
    "\n",
    "    return y_pred_gmm\n",
    "    for i in range(ncluster):\n",
    "        print(f'gmm: {np.sum(y_pred_gmm==i)}')\n",
    "\n",
    "\n",
    "\n",
    "# clustering_algorithms = (\n",
    "#     (\"MiniBatch\\nKMeans\", two_means),\n",
    "#     (\"Affinity\\nPropagation\", affinity_propagation),\n",
    "#     (\"MeanShift\", ms),\n",
    "#     (\"Spectral\\nClustering\", spectral),\n",
    "#     (\"Ward\", ward),\n",
    "#     (\"Agglomerative\\nClustering\", average_linkage),\n",
    "#     (\"DBSCAN\", dbscan),\n",
    "#     (\"OPTICS\", optics),\n",
    "#     (\"BIRCH\", birch),\n",
    "#     (\"Gaussian\\nMixture\", gmm),\n",
    "# )\n",
    "\n",
    "# for name, algorithm in clustering_algorithms:\n",
    "\n",
    "#     # catch warnings related to kneighbors_graph\n",
    "#     with warnings.catch_warnings():\n",
    "#         warnings.filterwarnings(\n",
    "#             \"ignore\",\n",
    "#             message=\"the number of connected components of the \"\n",
    "#             + \"connectivity matrix is [0-9]{1,2}\"\n",
    "#             + \" > 1. Completing it to avoid stopping the tree early.\",\n",
    "#             category=UserWarning,\n",
    "#         )\n",
    "#         warnings.filterwarnings(\n",
    "#             \"ignore\",\n",
    "#             message=\"Graph is not fully connected, spectral embedding\"\n",
    "#             + \" may not work as expected.\",\n",
    "#             category=UserWarning,\n",
    "#         )\n",
    "#         algorithm.fit(X_cluster)\n",
    "\n",
    "#     if hasattr(algorithm, \"labels_\"):\n",
    "#         y_pred = algorithm.labels_.astype(int)\n",
    "#     else:\n",
    "#         y_pred = algorithm.predict(X_cluster)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('p_eCluster', 147800.2201810238),\n",
       " ('p_rawECluster', 136551.60193191108),\n",
       " ('p_eAccCluster', 132768.7296983104),\n",
       " ('p_ecore', 128218.3746562804),\n",
       " ('p_E7x11_Lr2', 94547.8555268207),\n",
       " ('p_e277', 93432.63369398437),\n",
       " ('p_E7x7_Lr2', 93140.09870607701),\n",
       " ('p_eClusterLr2', 92209.85766527813)]"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from kmeans_interp.kmeans_feature_imp import KMeansInterp\n",
    "\n",
    "kms = KMeansInterp(\n",
    "\tn_clusters=3,\n",
    "\tordered_feature_names=X.columns.tolist(), \n",
    "\tfeature_importance_method='wcss_min', # or 'unsup2sup'\n",
    ").fit(X.values)\n",
    "\n",
    "kms.feature_importances_[0][:8]\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4e3119b6b440005e83014445b502bc062a01c9850f9c4ea1b0d68db6d948f423"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
