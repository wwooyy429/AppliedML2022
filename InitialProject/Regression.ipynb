{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import pandas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import tree\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.metrics import accuracy_score, r2_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "import shap\n",
    "import lightgbm as lgb \n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from optuna.integration import LightGBMPruningCallback\n",
    "from optuna.pruners import MedianPruner\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.metrics import mean_absolute_error \n",
    "from sklearn.feature_selection import SelectKBest, f_regression, chi2, f_classif, mutual_info_regression\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training data set: (162500, 166)\n",
      "Shape of test data set: (160651, 164)\n",
      "[-0.41764802 -0.01699412  0.01510229 ... -0.29044643 -0.21449289\n",
      " -1.1436536 ]\n",
      "Shape of X: (121495, 160)\n",
      "Shape of y: (121495,)\n"
     ]
    }
   ],
   "source": [
    "# read data\n",
    "def load_data(name):\n",
    "    with h5py.File(f'{name}.h5', 'r') as f:\n",
    "        filename = name.split('/')[-1]\n",
    "        return pandas.DataFrame(f[filename][:], dtype=np.float64)\n",
    "\n",
    "train = load_data('train')\n",
    "test  = load_data('test')\n",
    "\n",
    "print (f'Shape of training data set: {train.shape}')\n",
    "print (f'Shape of test data set: {test.shape}')\n",
    "\n",
    "all_variables = ['actualInteractionsPerCrossing', 'averageInteractionsPerCrossing', 'correctedActualMu', 'correctedAverageMu', 'correctedScaledActualMu', 'correctedScaledAverageMu', 'NvtxReco', 'p_nTracks', 'p_pt_track', 'p_eta', 'p_phi', 'p_charge', 'p_qOverP', 'p_z0', 'p_d0', 'p_sigmad0', 'p_d0Sig', 'p_EptRatio', 'p_dPOverP', 'p_z0theta', 'p_etaCluster', 'p_phiCluster', 'p_eCluster', 'p_rawEtaCluster', 'p_rawPhiCluster', 'p_rawECluster', 'p_eClusterLr0', 'p_eClusterLr1', 'p_eClusterLr2', 'p_eClusterLr3', 'p_etaClusterLr1', 'p_etaClusterLr2', 'p_phiClusterLr2', 'p_eAccCluster', 'p_f0Cluster', 'p_etaCalo', 'p_phiCalo', 'p_eTileGap3Cluster', 'p_cellIndexCluster', 'p_phiModCalo', 'p_etaModCalo', 'p_dPhiTH3', 'p_R12', 'p_fTG3', 'p_weta2', 'p_Reta', 'p_Rphi', 'p_Eratio', 'p_f1', 'p_f3', 'p_Rhad', 'p_Rhad1', 'p_deltaEta1', 'p_deltaPhiRescaled2', 'p_TRTPID', 'p_TRTTrackOccupancy', 'p_numberOfInnermostPixelHits', 'p_numberOfPixelHits', 'p_numberOfSCTHits', 'p_numberOfTRTHits', 'p_numberOfTRTXenonHits', 'p_chi2', 'p_ndof', 'p_SharedMuonTrack', 'p_E7x7_Lr2', 'p_E7x7_Lr3', 'p_E_Lr0_HiG', 'p_E_Lr0_LowG', 'p_E_Lr0_MedG', 'p_E_Lr1_HiG', 'p_E_Lr1_LowG', 'p_E_Lr1_MedG', 'p_E_Lr2_HiG', 'p_E_Lr2_LowG', 'p_E_Lr2_MedG', 'p_E_Lr3_HiG', 'p_E_Lr3_LowG', 'p_E_Lr3_MedG', 'p_ambiguityType', 'p_asy1', 'p_author', 'p_barys1', 'p_core57cellsEnergyCorrection', 'p_deltaEta0', 'p_deltaEta2', 'p_deltaEta3', 'p_deltaPhi0', 'p_deltaPhi1', 'p_deltaPhi2', 'p_deltaPhi3', 'p_deltaPhiFromLastMeasurement', 'p_deltaPhiRescaled0', 'p_deltaPhiRescaled1', 'p_deltaPhiRescaled3', 'p_e1152', 'p_e132', 'p_e235', 'p_e255', 'p_e2ts1', 'p_ecore', 'p_emins1', 'p_etconeCorrBitset', 'p_ethad', 'p_ethad1', 'p_f1core', 'p_f3core', 'p_maxEcell_energy', 'p_maxEcell_gain', 'p_maxEcell_time', 'p_maxEcell_x', 'p_maxEcell_y', 'p_maxEcell_z', 'p_nCells_Lr0_HiG', 'p_nCells_Lr0_LowG', 'p_nCells_Lr0_MedG', 'p_nCells_Lr1_HiG', 'p_nCells_Lr1_LowG', 'p_nCells_Lr1_MedG', 'p_nCells_Lr2_HiG', 'p_nCells_Lr2_LowG', 'p_nCells_Lr2_MedG', 'p_nCells_Lr3_HiG', 'p_nCells_Lr3_LowG', 'p_nCells_Lr3_MedG', 'p_pos', 'p_pos7', 'p_poscs1', 'p_poscs2', 'p_ptconeCorrBitset', 'p_ptconecoreTrackPtrCorrection', 'p_r33over37allcalo', 'p_topoetconeCorrBitset', 'p_topoetconecoreConeEnergyCorrection', 'p_topoetconecoreConeSCEnergyCorrection', 'p_weta1', 'p_widths1', 'p_widths2', 'p_wtots1', 'p_e233', 'p_e237', 'p_e277', 'p_e2tsts1', 'p_ehad1', 'p_emaxs1', 'p_fracs1', 'p_DeltaE', 'p_E3x5_Lr0', 'p_E3x5_Lr1', 'p_E3x5_Lr2', 'p_E3x5_Lr3', 'p_E5x7_Lr0', 'p_E5x7_Lr1', 'p_E5x7_Lr2', 'p_E5x7_Lr3', 'p_E7x11_Lr0', 'p_E7x11_Lr1', 'p_E7x11_Lr2', 'p_E7x11_Lr3', 'p_E7x7_Lr0', 'p_E7x7_Lr1' ]\n",
    "\n",
    "all = train[all_variables]\n",
    "\n",
    "electron_truth = train['Truth']\n",
    "y = train['p_truth_E']\n",
    "y = y[electron_truth==1]\n",
    "X = train[all_variables]\n",
    "X = X[electron_truth==1]\n",
    "y = y.to_numpy(dtype='float32')\n",
    "\n",
    "# print(y)\n",
    "# prepocessing\n",
    "\n",
    "norm = preprocessing.MinMaxScaler()\n",
    "sc_X = preprocessing.StandardScaler()\n",
    "sc_y = preprocessing.StandardScaler()\n",
    "X = sc_X.fit_transform(X)\n",
    "y = np.reshape(y, (-1,1))\n",
    "y = sc_y.fit_transform(y)\n",
    "y = y.ravel()\n",
    "\n",
    "print(y)\n",
    "\n",
    "X = pd.DataFrame(X, columns=all_variables)\n",
    "# y = pd.DataFrame(y, columns=['p_truth_E'])\n",
    "\n",
    "\n",
    "print (f'Shape of X: {X.shape}')\n",
    "print (f'Shape of y: {y.shape}')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_selection\n",
    "def selection_features(X_train, y_train, X_test):\n",
    "    select = SelectKBest(score_func=mutual_info_regression, k=12)\n",
    "    select.fit(X_train, y_train)\n",
    "    \n",
    "    return select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select = selection_features(X_train, y_train, X_test)\n",
    "# X = pd.DataFrame(X)\n",
    "# names = X.columns.values[select.get_support()]\n",
    "# scores = select.scores_[select.get_support()]\n",
    "# names_scores = list(zip(names, scores))\n",
    "# ns_df = pd.DataFrame(data = names_scores, columns=['Feature_names', 'Feature_scores'])\n",
    "# #Sort the dataframe for better visualization\n",
    "# ns_df_sorted = ns_df.sort_values(['Feature_names', 'Feature_scores'], ascending = [False, True])\n",
    "# print(ns_df_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_feature_importance():\n",
    "#     model = SelectKBest(mutual_info_regression, k=12)#选择k个最佳特征\n",
    "#     X_new = model.fit_transform(X, y)\n",
    "#     #feature_data是特征数据，label_data是标签数据，该函数可以选择出k个特征 \n",
    " \n",
    "#     print(\"model shape: \",X_new.shape)\n",
    " \n",
    "#     scores = model.scores_\n",
    "#     print('model scores:', scores)  # 得分越高，特征越重要\n",
    " \n",
    "#     p_values = model.pvalues_\n",
    "#     print('model p-values', p_values)  # p-values 越小，置信度越高，特征越重要\n",
    " \n",
    "#     # 按重要性排序，选出最重要的 k 个\n",
    "#     indices = np.argsort(scores)[::-1]\n",
    "#     k_best_features = list(X.columns.values[indices[0:12]])\n",
    " \n",
    "#     print('k best features are: ',k_best_features)\n",
    "    \n",
    "#     return k_best_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "kbest_variables = ['p_eCluster',\n",
    " 'p_rawECluster',\n",
    " 'p_eAccCluster',\n",
    " 'p_ecore',\n",
    " 'p_E7x11_Lr2',\n",
    " 'p_E7x7_Lr2',\n",
    " 'p_e277',\n",
    " 'p_eClusterLr2',\n",
    " 'p_E5x7_Lr2',\n",
    " 'p_e255',\n",
    " 'p_e237',\n",
    " 'p_e235']\n",
    "\n",
    "\n",
    "sc_X_kbest = preprocessing.StandardScaler()\n",
    "X_kbest = train[kbest_variables]\n",
    "X_kbest = X_kbest[electron_truth==1]\n",
    "X_kbest = sc_X_kbest.fit_transform(X_kbest)\n",
    "X_kbest = pd.DataFrame(X_kbest, columns=kbest_variables)\n",
    "\n",
    "sc_y_kbest = preprocessing.StandardScaler()\n",
    "y_kbest = train['p_truth_E']\n",
    "y_kbest = y_kbest[electron_truth==1]\n",
    "y_kbest = y_kbest.to_numpy(dtype='float32')\n",
    "y_kbest = np.reshape(y_kbest, (-1,1))\n",
    "y_kbest = sc_y_kbest.fit_transform(y_kbest)\n",
    "y_kbest = y_kbest.ravel()\n",
    "\n",
    "X_kbest_train, X_kbest_test, y_kbest_train, y_kbest_test = train_test_split(X_kbest, y_kbest, test_size=0.2, random_state=12)\n",
    "\n",
    "sc_input_kbest = preprocessing.StandardScaler()\n",
    "input_valid_kbest= test[kbest_variables]\n",
    "input_valid_kbest = sc_input_kbest.fit_transform(input_valid_kbest)\n",
    "input_valid_kbest = pd.DataFrame(input_valid_kbest, columns=[kbest_variables])\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_variables = ['p_eCluster', 'p_eAccCluster', 'p_nCells_Lr1_HiG', 'p_ecore', 'p_EptRatio', 'p_rawECluster', 'p_d0', 'p_E3x5_Lr1', 'p_nTracks', 'p_deltaEta2', 'p_pt_track', 'p_deltaPhi2' ]\n",
    "\n",
    "X_shap = train[shap_variables]\n",
    "X_shap = X_shap[electron_truth==1]\n",
    "X_shap = sc_X.fit_transform(X_shap)\n",
    "X_shap = pd.DataFrame(X_shap, columns=shap_variables)\n",
    "\n",
    "sc_y_shap = preprocessing.StandardScaler()\n",
    "y_shap = train['p_truth_E']\n",
    "y_shap = y_shap[electron_truth==1]\n",
    "y_shap = y_shap.to_numpy(dtype='float32')\n",
    "y_shap = np.reshape(y_shap, (-1,1))\n",
    "y_shap = sc_y_shap.fit_transform(y_shap)\n",
    "y_shap = y_shap.ravel()\n",
    "\n",
    "X_shap_train, X_shap_test, y_shap_train, y_shap_test = train_test_split(X_shap, y_shap, test_size=0.2, random_state=12)\n",
    "\n",
    "sc_input_shap = preprocessing.StandardScaler()\n",
    "input_valid_shap = test[shap_variables]\n",
    "input_valid_shap = sc_input_shap.fit_transform(input_valid_shap)\n",
    "input_valid_shap = pd.DataFrame(input_valid_shap, columns=[shap_variables])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_variables = ['p_eCluster', 'p_deltaEta2', 'p_deltaPhiRescaled2', 'p_deltaEta1', 'p_EptRatio', 'p_d0', 'p_deltaPhi2', 'p_sigmad0', 'p_ecore', 'p_qOverP', 'p_pt_track', 'p_DeltaE']\n",
    "X_lgb = train[lgb_variables]\n",
    "X_lgb = X_lgb[electron_truth==1]\n",
    "X_lgb = sc_X.fit_transform(X_lgb)\n",
    "X_lgb = pd.DataFrame(X_lgb, columns=lgb_variables)\n",
    "\n",
    "sc_y_lgb = preprocessing.StandardScaler()\n",
    "y_lgb = train['p_truth_E']\n",
    "y_lgb= y_lgb[electron_truth==1]\n",
    "y_lgb = y_lgb.to_numpy(dtype='float32')\n",
    "y_lgb = np.reshape(y_lgb, (-1,1))\n",
    "y_lgb = sc_y_lgb.fit_transform(y_lgb)\n",
    "y_lgb = y_lgb.ravel()\n",
    "\n",
    "X_lgb_train, X_lgb_test, y_lgb_train, y_lgb_test = train_test_split(X_lgb, y_lgb, test_size=0.2, random_state=12)\n",
    "sc_input_lgb = preprocessing.StandardScaler()\n",
    "input_valid_lgb = test[lgb_variables]\n",
    "input_valid_lgb = sc_input_lgb.fit_transform(input_valid_lgb)\n",
    "input_valid_lgb = pd.DataFrame(input_valid_lgb, columns=[lgb_variables])\n",
    "\n",
    "variable_list_shap = pd.DataFrame(shap_variables, columns=['vars'])\n",
    "variable_list_shap.to_csv('solutions/Regression_HauLamFong_xgboost_VariableList.txt')\n",
    "variable_list_lgb = pd.DataFrame(lgb_variables, columns=['vars'])\n",
    "variable_list_lgb.to_csv('solutions/Regression_HauLamFong_lightgbm_VariableList.txt')\n",
    "variable_list_kbest = pd.DataFrame(kbest_variables, columns=['vars'])\n",
    "variable_list_kbest.to_csv('solutions/Regression_HauLamFong_neuralnetwork_VariableList.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cols</th>\n",
       "      <th>feature_imp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>p_eCluster</td>\n",
       "      <td>264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>p_deltaEta2</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>p_deltaPhiRescaled2</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>p_deltaEta1</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>p_EptRatio</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>p_d0</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>p_deltaPhi2</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>p_sigmad0</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>p_ecore</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>p_qOverP</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>p_pt_track</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>p_DeltaE</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    cols  feature_imp\n",
       "22            p_eCluster          264\n",
       "84           p_deltaEta2          180\n",
       "53   p_deltaPhiRescaled2          131\n",
       "52           p_deltaEta1          125\n",
       "17            p_EptRatio           92\n",
       "14                  p_d0           91\n",
       "88           p_deltaPhi2           86\n",
       "15             p_sigmad0           79\n",
       "99               p_ecore           71\n",
       "12              p_qOverP           66\n",
       "8             p_pt_track           65\n",
       "145             p_DeltaE           65"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def feature_importance_lgb():\n",
    "    gbm = lgb.LGBMRegressor()\n",
    "    gbm.fit(X_train, y_train)\n",
    "    gbm.booster_.feature_importance()\n",
    "\n",
    "    feature_imp_ = pd.DataFrame({'cols':X_train.columns, 'feature_imp':gbm.feature_importances_})\n",
    "    feature_imp_.loc[feature_imp_.feature_imp > 0].sort_values(by=['feature_imp'], ascending=False)\n",
    "\n",
    "    return feature_imp_.loc[feature_imp_.feature_imp > 0].sort_values(by=['feature_imp'], ascending=False).head(12)\n",
    "\n",
    "feature_importance_lgb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "# predict\n",
    "\n",
    "# optimize hyperparameters of lgb\n",
    "\n",
    "# def objective_lgb(trial):\n",
    "\n",
    "#     X_lgb_train, X_lgb_test, y_lgb_train, y_lgb_test = train_test_split(X_lgb, y, test_size=0.2, random_state=42)\n",
    "#     train_data = lgb.Dataset(X_lgb_train, label=y_lgb_train)\n",
    "#     valid_data = lgb.Dataset(X_lgb_test, label=y_lgb_test)\n",
    "    \n",
    "#     boosting_types = [\"gbdt\", \"rf\", \"dart\"]\n",
    "#     boosting_type = trial.suggest_categorical(\"boosting_type\", boosting_types)\n",
    "\n",
    "#     params = {\n",
    "#         'objective': 'regression',\n",
    "#         'boosting_type': trial.suggest_categorical(\"boosting_type\", [\"gbdt\", \"rf\", \"dart\"]),\n",
    "#         'max_depth': trial.suggest_int('max_depth', 2, 100),\n",
    "#         'min_child_samples': trial.suggest_int('min_child_samples', 0, 1000),\n",
    "#         'metric': 'mae',\n",
    "#         'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.5),\n",
    "#         'feature_fraction': trial.suggest_float('feature_fraction', 0.2, 0.95, step=0.1),\n",
    "#         'num_leaves': trial.suggest_int('num_leaves', 20, 3000, step=20),\n",
    "#         'bagging_freq': trial.suggest_categorical('bagging_freq', [1]),\n",
    "#         'bagging_fraction': trial.suggest_float('bagging_fraction', 0.2, 0.95, step=0.1),\n",
    "#         'reg_alpha': trial.suggest_float(\"reg_alpha\", 0, 100, step=0.1),\n",
    "#         'reg_lambda': trial.suggest_int(\"reg_lambda\", 0, 1000, step=1),\n",
    "#         'verbosity': -1,\n",
    "#     }\n",
    "\n",
    "#     lgbm = LGBMRegressor(**params)\n",
    "#     lgbm.fit(X_lgb_train, y_lgb_train, eval_set=[(X_lgb_test, y_lgb_test)],early_stopping_rounds=100, verbose=False)\n",
    "#     pred_lgb=lgbm.predict(X_lgb_test)\n",
    "#     mae = mean_absolute_error(y_lgb_test, pred_lgb)\n",
    "#     return mae\n",
    "\n",
    "\n",
    "# study = optuna.create_study(\n",
    "#     direction=\"minimize\",\n",
    "#     sampler=TPESampler(seed=42),\n",
    "#     pruner=MedianPruner(n_warmup_steps=50),\n",
    "# ) \n",
    "\n",
    "# study.optimize(objective_lgb, n_trials=100, show_progress_bar=True)\n",
    "\n",
    "# study.best_trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def optimized_lgb(X_train1, y_train1, X_test1, y_test1):\n",
    "\n",
    "#     # params = {\n",
    "#     #     'objective': 'regression',\n",
    "#     #     'metric': 'mae',\n",
    "#     #     'boosting_type': 'rf',\n",
    "#     #     'max_depth': 2,\n",
    "#     #     'min_child_samples': 74,\n",
    "#     #     'learning_rate': 0.4319464280296327,\n",
    "#     #     'feature_fraction': 0.9,\n",
    "#     #     'num_leaves': 680,\n",
    "#     #     'bagging_freq': 1,\n",
    "#     #     'bagging_fraction': 0.2,\n",
    "#     #     'reg_alpha': 63.5,\n",
    "#     #     'reg_lambda': 985,\n",
    "#     #     'verbosity': -1,\n",
    "#     # }\n",
    "\n",
    "#     params = {\n",
    "#         'boosting_type': 'gbdt',\n",
    "#         'max_depth': 44,\n",
    "#         'min_child_samples': 198,\n",
    "#         'learning_rate': 0.24942129794855278,\n",
    "#         'feature_fraction': 0.6000000000000001,\n",
    "#         'num_leaves': 1160,\n",
    "#         'bagging_freq': 1,\n",
    "#         'bagging_fraction': 0.9,\n",
    "#         'reg_alpha': 15.5,\n",
    "#         'reg_lambda': 75,\n",
    "#     }\n",
    "\n",
    "#     lgbm = LGBMRegressor(**params, num_boost_round=1000)\n",
    "#     lgbm.fit(X_train1, y_train1, eval_set=[(X_test1, y_test1)],early_stopping_rounds=100, verbose=False)\n",
    "#     pred_lgb=lgbm.predict(X_test1)\n",
    "#     mae = mean_absolute_error(y_test1, pred_lgb)\n",
    "#     r2s = r2_score(y_test1, pred_lgb)\n",
    "    \n",
    "#     # y_pred = lgbm.predict(X_test1.values)\n",
    "#     y_pred = pd.DataFrame(data=pred_lgb)\n",
    "#     y_pred = sc_y.inverse_transform(y_pred)\n",
    "    \n",
    "#     return mae, r2s, y_pred\n",
    "\n",
    "# optimized_lgb(X_lgb_train, y_lgb_train, X_lgb_test, y_lgb_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorboard.plugins.hparams import api as hp\n",
    "\n",
    "# HP_NUM_UNITS1 = hp.HParam('num_units1', hp.Discrete([16, 32, 64, 128]))\n",
    "# HP_NUM_UNITS2 = hp.HParam('num_units2', hp.Discrete([16, 32, 64, 128]))\n",
    "# HP_NUM_UNITS3 = hp.HParam('num_units3', hp.Discrete([16, 32, 64, 128]))\n",
    "# HP_DROPOUT = hp.HParam('dropout', hp.RealInterval(0.1, 0.2))\n",
    "\n",
    "# METRIC_ACCURACY = 'accuracy'\n",
    "\n",
    "# with tf.summary.create_file_writer('logs/hparam_tuning').as_default():\n",
    "#   hp.hparams_config(\n",
    "#     hparams=[HP_NUM_UNITS1, HP_NUM_UNITS2, HP_NUM_UNITS3],\n",
    "#     metrics=[hp.Metric(METRIC_ACCURACY, display_name='Accuracy')],\n",
    "#   )\n",
    "\n",
    "# def train_test_model(hparams):\n",
    "#   model = tf.keras.models.Sequential([\n",
    "#     tf.keras.layers.Dense(hparams[HP_NUM_UNITS1], activation=tf.nn.relu),\n",
    "#     tf.keras.layers.Dense(hparams[HP_NUM_UNITS2], activation=tf.nn.relu),\n",
    "#     tf.keras.layers.Dense(hparams[HP_NUM_UNITS3], activation=tf.nn.relu),\n",
    "#     tf.keras.layers.Dense(1),\n",
    "#   ])\n",
    "#   model.compile(\n",
    "#       optimizer='adam',\n",
    "#       loss='mae',\n",
    "#       metrics=['mae'],\n",
    "#   )\n",
    "\n",
    "#   model.fit(X_kbest_train, y_kbest_train, epochs=5) # Run with 1 epoch to speed things up for demo purposes\n",
    "#   _, accuracy = model.evaluate(X_kbest_test, y_kbest_test)\n",
    "#   return accuracy\n",
    "\n",
    "# def run(run_dir, hparams):\n",
    "#   with tf.summary.create_file_writer(run_dir).as_default():\n",
    "#     hp.hparams(hparams)  # record the values used in this trial\n",
    "#     accuracy = train_test_model(hparams)\n",
    "#     tf.summary.scalar(METRIC_ACCURACY, accuracy, step=1)\n",
    "\n",
    "# session_num = 0\n",
    "\n",
    "# for num_units1 in HP_NUM_UNITS1.domain.values:\n",
    "#     for num_units2 in HP_NUM_UNITS2.domain.values:\n",
    "#       for num_units3 in HP_NUM_UNITS3.domain.values:\n",
    "#             hparams = {\n",
    "#                 HP_NUM_UNITS1: num_units1,\n",
    "#                 HP_NUM_UNITS2: num_units2,\n",
    "#                 HP_NUM_UNITS3: num_units3\n",
    "#             }\n",
    "#             run_name = \"run-%d\" % session_num\n",
    "#             print('--- Starting trial: %s' % run_name)\n",
    "#             print({h.name: hparams[h] for h in hparams})\n",
    "#             run('logs/hparam_tuning/' + run_name, hparams)\n",
    "#             session_num += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 16000), started 16:58:01 ago. (Use '!kill 16000' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-28bbc8c741bbd82c\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-28bbc8c741bbd82c\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 'num_units1': 64, 'num_units2': 32, 'num_units3': 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # xgboost\n",
    "# import xgboost\n",
    "\n",
    "# def shap_xgbooster():\n",
    "#     model = xgboost.XGBRegressor().fit(X, y)\n",
    "#     explainer = shap.Explainer(model)\n",
    "#     shap_values = explainer(X)\n",
    "\n",
    "#     feature_names = shap_values.feature_names\n",
    "#     shap_df = pd.DataFrame(shap_values.values, columns=feature_names)\n",
    "#     vals = np.abs(shap_df.values).mean(0)\n",
    "#     shap_importance = pd.DataFrame(list(zip(feature_names, vals)), columns=['col_name', 'feature_importance_vals'])\n",
    "#     shap_importance.sort_values(by=['feature_importance_vals'], ascending=False, inplace=True)\n",
    "#     shap.plots.bar(shap_values)\n",
    "\n",
    "#     return shap_importance.head(20)\n",
    "\n",
    "# shap_xgbooster()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# def objective_xgboost(trial):\n",
    "    \n",
    "#     params = {\n",
    "#     'n_estimators':trial.suggest_int('n_estimators', 0, 100),\n",
    "#     'verbosity': 0,\n",
    "#     'reg_alpha':trial.suggest_int('reg_alpha', 0, 5),\n",
    "#     'reg_lambda':trial.suggest_int('reg_lambda', 0, 5),\n",
    "#     'min_child_weight':trial.suggest_int('min_child_weight', 0, 100), \n",
    "#     'gamma':trial.suggest_int('gamma', 0, 6),  \n",
    "#     'subsample':trial.suggest_discrete_uniform('subsample', 0.1, 1, 0.01),\n",
    "#     'colsample_bytree':trial.suggest_discrete_uniform('colsample_bytree', 0.1, 1, 0.01), \n",
    "#     'max_depth': trial.suggest_int('max_depth', 3, 100),\n",
    "#     'objective': trial.suggest_categorical('objective', ['reg:squarederror']),\n",
    "#     'booster': trial.suggest_categorical('booster',['gbtree', 'gblinear', 'dart']),\n",
    "#     'eval_metric': ['logloss'],\n",
    "#     'learning_rate': trial.suggest_loguniform('learning_rate', 0.001, 1),\n",
    "# }\n",
    "\n",
    "#     model = xgboost.XGBRegressor(**params)\n",
    "#     model.fit(X_shap_train,y_shap_train, eval_set=[(X_shap_test, y_shap_test)], early_stopping_rounds=50, verbose=False)\n",
    "#     preds = model.predict(X_shap_test)\n",
    "#     rmse = mean_squared_error(y_shap_test, preds, squared=False)\n",
    "\n",
    "#     return rmse\n",
    "\n",
    "# ntrial = 50\n",
    "# study = optuna.create_study(direction='minimize', sampler=TPESampler(seed=42), pruner=MedianPruner(n_warmup_steps=50))\n",
    "# study.optimize(objective_xgboost, n_trials=ntrial, show_progress_bar=True)\n",
    "\n",
    "# print('Best trial: score {},\\nparams {}'.format(study.best_trial.value,study.best_trial.params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 85022.66 ,  85153.305, 233904.77 , ..., 123212.17 ,  83450.37 ,\n",
      "       158029.22 ], dtype=float32), 0.9218083512407435, 0.14025287)\n"
     ]
    }
   ],
   "source": [
    "def optimized_xgboost(X_train1, y_train1, X_test1, y_test1, input_valid_shap):\n",
    "    params = {'n_estimators': 100, 'reg_alpha': 3, 'reg_lambda': 0, 'min_child_weight': 29, 'gamma': 3, 'subsample': 0.99, \n",
    "    'colsample_bytree': 0.82, 'max_depth': 38, 'objective': 'reg:squarederror', 'booster': 'gbtree', 'learning_rate': 0.09140602270486103}\n",
    "    model = xgboost.XGBRegressor(**params)\n",
    "    model.fit(X_train1, y_train1)\n",
    "    y_pred_xgboost = model.predict(input_valid_shap)\n",
    "    y_estimate = model.predict(X_test1)\n",
    "    r2s = r2_score(y_test1, y_estimate)\n",
    "    mae = mean_absolute_error(y_test1, y_estimate)\n",
    "    y_pred = sc_y.inverse_transform(y_pred_xgboost)\n",
    "\n",
    "    return y_pred, r2s, mae\n",
    "\n",
    "pred_xgboost = optimized_xgboost(X_shap_train, y_shap_train, X_shap_test, y_shap_test, input_valid_shap)\n",
    "print(pred_xgboost)\n",
    "\n",
    "solution_xg = pd.DataFrame(data=pred_xgboost[0], columns=['preds'])\n",
    "solution_xg.to_csv('solutions/Regression_HauLamFong_xgboost.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- TRAINING ---------\n",
      "Epoch 1/10\n",
      "190/190 [==============================] - 1s 2ms/step - loss: 0.1818 - mse: 0.1384 - val_loss: 0.1400 - val_mse: 0.0910\n",
      "Epoch 2/10\n",
      "190/190 [==============================] - 0s 2ms/step - loss: 0.1418 - mse: 0.1002 - val_loss: 0.1425 - val_mse: 0.0934\n",
      "Epoch 3/10\n",
      "190/190 [==============================] - 0s 2ms/step - loss: 0.1412 - mse: 0.0995 - val_loss: 0.1349 - val_mse: 0.0882\n",
      "Epoch 4/10\n",
      "190/190 [==============================] - 0s 1ms/step - loss: 0.1401 - mse: 0.0988 - val_loss: 0.1351 - val_mse: 0.0884\n",
      "Epoch 5/10\n",
      "190/190 [==============================] - 0s 1ms/step - loss: 0.1391 - mse: 0.0981 - val_loss: 0.1329 - val_mse: 0.0871\n",
      "Epoch 6/10\n",
      "190/190 [==============================] - 0s 1ms/step - loss: 0.1388 - mse: 0.0977 - val_loss: 0.1322 - val_mse: 0.0872\n",
      "Epoch 7/10\n",
      "190/190 [==============================] - 0s 1ms/step - loss: 0.1387 - mse: 0.0975 - val_loss: 0.1320 - val_mse: 0.0872\n",
      "Epoch 8/10\n",
      "190/190 [==============================] - 0s 1ms/step - loss: 0.1386 - mse: 0.0975 - val_loss: 0.1324 - val_mse: 0.0868\n",
      "Epoch 9/10\n",
      "190/190 [==============================] - 0s 1ms/step - loss: 0.1377 - mse: 0.0970 - val_loss: 0.1323 - val_mse: 0.0874\n",
      "Epoch 10/10\n",
      "190/190 [==============================] - 0s 1ms/step - loss: 0.1369 - mse: 0.0967 - val_loss: 0.1329 - val_mse: 0.0885\n",
      "760/760 - 0s - loss: 0.1329 - mse: 0.0885 - 341ms/epoch - 449us/step\n",
      "760/760 [==============================] - 0s 510us/step\n",
      "5021/5021 [==============================] - 3s 609us/step\n",
      "([0.13293737173080444, 0.08847861737012863], 0.13293721, array([[ 87428.82 ],\n",
      "       [ 90313.2  ],\n",
      "       [233869.45 ],\n",
      "       ...,\n",
      "       [126616.06 ],\n",
      "       [109500.055],\n",
      "       [157651.77 ]], dtype=float32), 0.9095613668092621)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# neural network\n",
    "\n",
    "def optimized_nn(X_train1, y_train1, X_test1, y_test1, input_valid_kbest):\n",
    "    model = Sequential([\n",
    "        Dense(64,activation='relu',name='input_layer'),\n",
    "        Dense(32,activation='relu',name='hidden_layer1'),\n",
    "        Dense(128,activation='relu',name='hidden_layer2'),\n",
    "        Dense(1, name='output')])\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam()\n",
    "    model.compile(optimizer=optimizer,\n",
    "                loss='mae',\n",
    "                metrics='mse')\n",
    "\n",
    "    print('--------- TRAINING ---------')\n",
    "    history = model.fit(x=X_train1, y=y_train1, validation_data=(X_test1, y_test1), epochs = 10, batch_size=512)  \n",
    "    score = model.evaluate(X_test1,  y_test1, verbose=2)\n",
    "    y_estimate = model.predict(X_test1)\n",
    "    y_pred_nn = model.predict(input_valid_kbest)\n",
    "    mae = mean_absolute_error(y_test1, y_estimate)\n",
    "    y_pred1 = sc_y.inverse_transform(y_pred_nn)\n",
    "\n",
    "    return score, mae, y_pred1, r2_score(y_test1, y_estimate)\n",
    "\n",
    "pred_nn = optimized_nn(X_kbest_train, y_kbest_train, X_kbest_test, y_kbest_test, input_valid_kbest)\n",
    "print(pred_nn)\n",
    "\n",
    "solution_nn = pd.DataFrame(data=pred_nn[2], columns=['preds'])\n",
    "solution_nn.to_csv('solutions/Regression_HauLamFong_neuralnetwork.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002359 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3060\n",
      "[LightGBM] [Info] Number of data points in the train set: 97196, number of used features: 12\n",
      "[LightGBM] [Info] Start training from score -0.000420\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1]\tvalid_0's l2: 0.599098\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2]\tvalid_0's l2: 0.382491\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3]\tvalid_0's l2: 0.264634\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4]\tvalid_0's l2: 0.190068\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[5]\tvalid_0's l2: 0.146842\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[6]\tvalid_0's l2: 0.124009\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[7]\tvalid_0's l2: 0.108298\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[8]\tvalid_0's l2: 0.10496\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[9]\tvalid_0's l2: 0.0967824\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[10]\tvalid_0's l2: 0.0927226\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[11]\tvalid_0's l2: 0.0892654\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[12]\tvalid_0's l2: 0.086902\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[13]\tvalid_0's l2: 0.0852697\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[14]\tvalid_0's l2: 0.0838713\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[15]\tvalid_0's l2: 0.083058\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[16]\tvalid_0's l2: 0.0821534\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[17]\tvalid_0's l2: 0.0810891\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[18]\tvalid_0's l2: 0.0803855\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[19]\tvalid_0's l2: 0.0798948\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[20]\tvalid_0's l2: 0.0795865\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[21]\tvalid_0's l2: 0.078862\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[22]\tvalid_0's l2: 0.0784568\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[23]\tvalid_0's l2: 0.0780394\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[24]\tvalid_0's l2: 0.0777935\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[25]\tvalid_0's l2: 0.0774927\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[26]\tvalid_0's l2: 0.0772922\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[27]\tvalid_0's l2: 0.07694\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[28]\tvalid_0's l2: 0.0767796\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[29]\tvalid_0's l2: 0.0766265\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[30]\tvalid_0's l2: 0.0765295\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[31]\tvalid_0's l2: 0.0764345\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[32]\tvalid_0's l2: 0.0763371\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[33]\tvalid_0's l2: 0.0762659\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[34]\tvalid_0's l2: 0.076233\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[35]\tvalid_0's l2: 0.0761175\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[36]\tvalid_0's l2: 0.0760663\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[37]\tvalid_0's l2: 0.0760305\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[38]\tvalid_0's l2: 0.0759956\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[39]\tvalid_0's l2: 0.0759344\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[40]\tvalid_0's l2: 0.0758855\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[41]\tvalid_0's l2: 0.0758675\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[42]\tvalid_0's l2: 0.0757901\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[43]\tvalid_0's l2: 0.0757885\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[44]\tvalid_0's l2: 0.0757428\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[45]\tvalid_0's l2: 0.0757563\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[46]\tvalid_0's l2: 0.0757456\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[47]\tvalid_0's l2: 0.0757059\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[48]\tvalid_0's l2: 0.0756689\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[49]\tvalid_0's l2: 0.0756046\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[50]\tvalid_0's l2: 0.0755784\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[51]\tvalid_0's l2: 0.0755628\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[52]\tvalid_0's l2: 0.0755508\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[53]\tvalid_0's l2: 0.0755229\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[54]\tvalid_0's l2: 0.0754023\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[55]\tvalid_0's l2: 0.0753452\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[56]\tvalid_0's l2: 0.0753441\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[57]\tvalid_0's l2: 0.0753348\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[58]\tvalid_0's l2: 0.0752667\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[59]\tvalid_0's l2: 0.0752707\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[60]\tvalid_0's l2: 0.075192\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[61]\tvalid_0's l2: 0.0751848\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[62]\tvalid_0's l2: 0.0751842\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[63]\tvalid_0's l2: 0.0751872\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[64]\tvalid_0's l2: 0.0751939\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[65]\tvalid_0's l2: 0.0751393\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[66]\tvalid_0's l2: 0.0750676\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[67]\tvalid_0's l2: 0.0750545\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[68]\tvalid_0's l2: 0.0750312\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[69]\tvalid_0's l2: 0.0750383\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[70]\tvalid_0's l2: 0.0750188\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[71]\tvalid_0's l2: 0.0750293\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[72]\tvalid_0's l2: 0.0750204\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[73]\tvalid_0's l2: 0.0750221\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[74]\tvalid_0's l2: 0.0749873\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[75]\tvalid_0's l2: 0.0750018\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[76]\tvalid_0's l2: 0.0749901\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[77]\tvalid_0's l2: 0.074972\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[78]\tvalid_0's l2: 0.0749489\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[79]\tvalid_0's l2: 0.0749177\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[80]\tvalid_0's l2: 0.0749303\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[81]\tvalid_0's l2: 0.0749007\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[82]\tvalid_0's l2: 0.0748743\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[83]\tvalid_0's l2: 0.0748664\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[84]\tvalid_0's l2: 0.0748489\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[85]\tvalid_0's l2: 0.0748195\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[86]\tvalid_0's l2: 0.0748433\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[87]\tvalid_0's l2: 0.0748411\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[88]\tvalid_0's l2: 0.0748334\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[89]\tvalid_0's l2: 0.0748051\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[90]\tvalid_0's l2: 0.0747889\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[91]\tvalid_0's l2: 0.0747477\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[92]\tvalid_0's l2: 0.0747522\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[93]\tvalid_0's l2: 0.0748008\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[94]\tvalid_0's l2: 0.0747929\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[95]\tvalid_0's l2: 0.0747809\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[96]\tvalid_0's l2: 0.0747726\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[97]\tvalid_0's l2: 0.0747834\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[98]\tvalid_0's l2: 0.074779\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[99]\tvalid_0's l2: 0.0748212\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[100]\tvalid_0's l2: 0.0748165\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[101]\tvalid_0's l2: 0.0748117\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[102]\tvalid_0's l2: 0.0748224\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[103]\tvalid_0's l2: 0.0748471\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[104]\tvalid_0's l2: 0.0748403\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[105]\tvalid_0's l2: 0.0748307\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[106]\tvalid_0's l2: 0.0748635\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[107]\tvalid_0's l2: 0.0748636\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[108]\tvalid_0's l2: 0.0748698\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[109]\tvalid_0's l2: 0.0748629\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[110]\tvalid_0's l2: 0.0748783\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[111]\tvalid_0's l2: 0.0748863\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[112]\tvalid_0's l2: 0.0749003\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[113]\tvalid_0's l2: 0.0748819\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[114]\tvalid_0's l2: 0.0748837\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[115]\tvalid_0's l2: 0.0748831\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[116]\tvalid_0's l2: 0.0748713\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[117]\tvalid_0's l2: 0.0748547\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[118]\tvalid_0's l2: 0.0748703\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[119]\tvalid_0's l2: 0.0748572\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[120]\tvalid_0's l2: 0.0748554\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[121]\tvalid_0's l2: 0.0748191\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[122]\tvalid_0's l2: 0.074796\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[123]\tvalid_0's l2: 0.07478\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[124]\tvalid_0's l2: 0.0747614\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[125]\tvalid_0's l2: 0.0748035\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[126]\tvalid_0's l2: 0.0748151\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[127]\tvalid_0's l2: 0.074826\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[128]\tvalid_0's l2: 0.0748101\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[129]\tvalid_0's l2: 0.0748257\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[130]\tvalid_0's l2: 0.07484\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[131]\tvalid_0's l2: 0.0748129\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[132]\tvalid_0's l2: 0.0748199\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[133]\tvalid_0's l2: 0.0748166\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[134]\tvalid_0's l2: 0.0748246\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[135]\tvalid_0's l2: 0.0748205\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[136]\tvalid_0's l2: 0.0748171\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[137]\tvalid_0's l2: 0.0748385\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[138]\tvalid_0's l2: 0.0748359\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[139]\tvalid_0's l2: 0.0748448\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[140]\tvalid_0's l2: 0.0748734\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[141]\tvalid_0's l2: 0.0748513\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[142]\tvalid_0's l2: 0.0748287\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[143]\tvalid_0's l2: 0.0748377\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[144]\tvalid_0's l2: 0.0748655\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[145]\tvalid_0's l2: 0.0748716\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[146]\tvalid_0's l2: 0.0748736\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[147]\tvalid_0's l2: 0.0749092\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[148]\tvalid_0's l2: 0.0748903\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[149]\tvalid_0's l2: 0.0748916\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[150]\tvalid_0's l2: 0.0748855\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[151]\tvalid_0's l2: 0.0748754\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[152]\tvalid_0's l2: 0.0748581\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[153]\tvalid_0's l2: 0.0748678\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[154]\tvalid_0's l2: 0.0748631\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[155]\tvalid_0's l2: 0.0748669\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[156]\tvalid_0's l2: 0.0748569\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[157]\tvalid_0's l2: 0.0748542\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[158]\tvalid_0's l2: 0.0748627\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[159]\tvalid_0's l2: 0.0748619\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[160]\tvalid_0's l2: 0.0748401\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[161]\tvalid_0's l2: 0.0748326\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[162]\tvalid_0's l2: 0.0748166\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[163]\tvalid_0's l2: 0.0748219\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[164]\tvalid_0's l2: 0.0748321\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[165]\tvalid_0's l2: 0.0748313\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[166]\tvalid_0's l2: 0.0748658\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[167]\tvalid_0's l2: 0.0748729\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[168]\tvalid_0's l2: 0.0748704\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[169]\tvalid_0's l2: 0.0748476\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[170]\tvalid_0's l2: 0.0748581\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[171]\tvalid_0's l2: 0.0748598\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[172]\tvalid_0's l2: 0.0748561\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[173]\tvalid_0's l2: 0.0748578\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[174]\tvalid_0's l2: 0.0748989\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[175]\tvalid_0's l2: 0.0748956\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[176]\tvalid_0's l2: 0.0749027\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[177]\tvalid_0's l2: 0.0748987\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[178]\tvalid_0's l2: 0.0748812\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[179]\tvalid_0's l2: 0.0748783\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[180]\tvalid_0's l2: 0.0748853\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[181]\tvalid_0's l2: 0.0748714\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[182]\tvalid_0's l2: 0.074881\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[183]\tvalid_0's l2: 0.0748873\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[184]\tvalid_0's l2: 0.0748786\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[185]\tvalid_0's l2: 0.0748865\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[186]\tvalid_0's l2: 0.0749198\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[187]\tvalid_0's l2: 0.0749241\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[188]\tvalid_0's l2: 0.0749102\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[189]\tvalid_0's l2: 0.0749068\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[190]\tvalid_0's l2: 0.0749408\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[191]\tvalid_0's l2: 0.0749558\n",
      "Early stopping, best iteration is:\n",
      "[91]\tvalid_0's l2: 0.0747477\n",
      "(0.14333366960640773, 0.9235964225103069, array([ 81901.05743393,  85100.27308491, 229346.00666457, ...,\n",
      "       122818.09445177, 108739.36351329, 157713.93947594]))\n"
     ]
    }
   ],
   "source": [
    "def optimized_lgb(X_train1, y_train1, X_test1, y_test1, input_valid_lgb):\n",
    "\n",
    "    # params = {\n",
    "    #     'objective': 'regression',\n",
    "    #     'metric': 'mae',\n",
    "    #     'boosting_type': 'rf',\n",
    "    #     'max_depth': 2,\n",
    "    #     'min_child_samples': 74,\n",
    "    #     'learning_rate': 0.4319464280296327,\n",
    "    #     'feature_fraction': 0.9,\n",
    "    #     'num_leaves': 680,\n",
    "    #     'bagging_freq': 1,\n",
    "    #     'bagging_fraction': 0.2,\n",
    "    #     'reg_alpha': 63.5,\n",
    "    #     'reg_lambda': 985,\n",
    "    #     'verbosity': -1,\n",
    "    # }\n",
    "\n",
    "    params = {\n",
    "        'boosting_type': 'gbdt',\n",
    "        'max_depth': 44,\n",
    "        'min_child_samples': 198,\n",
    "        'learning_rate': 0.24942129794855278,\n",
    "        'feature_fraction': 0.6000000000000001,\n",
    "        'num_leaves': 1160,\n",
    "        'bagging_freq': 1,\n",
    "        'bagging_fraction': 0.9,\n",
    "        'reg_alpha': 15.5,\n",
    "        'reg_lambda': 75,\n",
    "    }\n",
    "\n",
    "    lgb_train = lgb.Dataset(X_train1, y_train1)\n",
    "    lgb_test = lgb.Dataset(X_test1, y_test1)\n",
    "\n",
    "    lgbm = lgb.train(params, lgb_train,num_boost_round=1000, valid_sets=lgb_test, early_stopping_rounds=100)\n",
    "    y_pred = lgbm.predict(input_valid_lgb)\n",
    "    pred_lgb=lgbm.predict(X_test1)\n",
    "    mae = mean_absolute_error(y_test1, pred_lgb)\n",
    "    r2s = r2_score(y_test1, pred_lgb)\n",
    "    \n",
    "    # y_pred = lgbm.predict(X_test1.values)\n",
    "    y_pred = sc_y.inverse_transform(y_pred)\n",
    "    \n",
    "    return mae, r2s, y_pred\n",
    "\n",
    "pred_lgb = optimized_lgb(X_lgb_train, y_lgb_train, X_lgb_test, y_lgb_test, input_valid_lgb)\n",
    "print(pred_lgb)\n",
    "\n",
    "solution_lgb = pd.DataFrame(data=pred_lgb[2], columns=['preds'])\n",
    "solution_lgb.to_csv('solutions/Regression_HauLamFong_lightgbm.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4e3119b6b440005e83014445b502bc062a01c9850f9c4ea1b0d68db6d948f423"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
