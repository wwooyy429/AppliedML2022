{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import pandas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import tree\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.metrics import accuracy_score, r2_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "import shap\n",
    "import lightgbm as lgb \n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from optuna.integration import LightGBMPruningCallback\n",
    "from optuna.pruners import MedianPruner\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.metrics import mean_absolute_error \n",
    "from sklearn.feature_selection import SelectKBest, f_regression, chi2, f_classif, mutual_info_regression\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training data set: (162500, 166)\n",
      "Shape of test data set: (160651, 164)\n",
      "[-0.41764802 -0.01699412  0.01510229 ... -0.29044643 -0.21449289\n",
      " -1.1436536 ]\n",
      "Shape of X: (121495, 160)\n",
      "Shape of y: (121495,)\n"
     ]
    }
   ],
   "source": [
    "# read data\n",
    "def load_data(name):\n",
    "    with h5py.File(f'{name}.h5', 'r') as f:\n",
    "        filename = name.split('/')[-1]\n",
    "        return pandas.DataFrame(f[filename][:], dtype=np.float64)\n",
    "\n",
    "train = load_data('train')\n",
    "test  = load_data('test')\n",
    "\n",
    "print (f'Shape of training data set: {train.shape}')\n",
    "print (f'Shape of test data set: {test.shape}')\n",
    "\n",
    "all_variables = ['actualInteractionsPerCrossing', 'averageInteractionsPerCrossing', 'correctedActualMu', 'correctedAverageMu', 'correctedScaledActualMu', 'correctedScaledAverageMu', 'NvtxReco', 'p_nTracks', 'p_pt_track', 'p_eta', 'p_phi', 'p_charge', 'p_qOverP', 'p_z0', 'p_d0', 'p_sigmad0', 'p_d0Sig', 'p_EptRatio', 'p_dPOverP', 'p_z0theta', 'p_etaCluster', 'p_phiCluster', 'p_eCluster', 'p_rawEtaCluster', 'p_rawPhiCluster', 'p_rawECluster', 'p_eClusterLr0', 'p_eClusterLr1', 'p_eClusterLr2', 'p_eClusterLr3', 'p_etaClusterLr1', 'p_etaClusterLr2', 'p_phiClusterLr2', 'p_eAccCluster', 'p_f0Cluster', 'p_etaCalo', 'p_phiCalo', 'p_eTileGap3Cluster', 'p_cellIndexCluster', 'p_phiModCalo', 'p_etaModCalo', 'p_dPhiTH3', 'p_R12', 'p_fTG3', 'p_weta2', 'p_Reta', 'p_Rphi', 'p_Eratio', 'p_f1', 'p_f3', 'p_Rhad', 'p_Rhad1', 'p_deltaEta1', 'p_deltaPhiRescaled2', 'p_TRTPID', 'p_TRTTrackOccupancy', 'p_numberOfInnermostPixelHits', 'p_numberOfPixelHits', 'p_numberOfSCTHits', 'p_numberOfTRTHits', 'p_numberOfTRTXenonHits', 'p_chi2', 'p_ndof', 'p_SharedMuonTrack', 'p_E7x7_Lr2', 'p_E7x7_Lr3', 'p_E_Lr0_HiG', 'p_E_Lr0_LowG', 'p_E_Lr0_MedG', 'p_E_Lr1_HiG', 'p_E_Lr1_LowG', 'p_E_Lr1_MedG', 'p_E_Lr2_HiG', 'p_E_Lr2_LowG', 'p_E_Lr2_MedG', 'p_E_Lr3_HiG', 'p_E_Lr3_LowG', 'p_E_Lr3_MedG', 'p_ambiguityType', 'p_asy1', 'p_author', 'p_barys1', 'p_core57cellsEnergyCorrection', 'p_deltaEta0', 'p_deltaEta2', 'p_deltaEta3', 'p_deltaPhi0', 'p_deltaPhi1', 'p_deltaPhi2', 'p_deltaPhi3', 'p_deltaPhiFromLastMeasurement', 'p_deltaPhiRescaled0', 'p_deltaPhiRescaled1', 'p_deltaPhiRescaled3', 'p_e1152', 'p_e132', 'p_e235', 'p_e255', 'p_e2ts1', 'p_ecore', 'p_emins1', 'p_etconeCorrBitset', 'p_ethad', 'p_ethad1', 'p_f1core', 'p_f3core', 'p_maxEcell_energy', 'p_maxEcell_gain', 'p_maxEcell_time', 'p_maxEcell_x', 'p_maxEcell_y', 'p_maxEcell_z', 'p_nCells_Lr0_HiG', 'p_nCells_Lr0_LowG', 'p_nCells_Lr0_MedG', 'p_nCells_Lr1_HiG', 'p_nCells_Lr1_LowG', 'p_nCells_Lr1_MedG', 'p_nCells_Lr2_HiG', 'p_nCells_Lr2_LowG', 'p_nCells_Lr2_MedG', 'p_nCells_Lr3_HiG', 'p_nCells_Lr3_LowG', 'p_nCells_Lr3_MedG', 'p_pos', 'p_pos7', 'p_poscs1', 'p_poscs2', 'p_ptconeCorrBitset', 'p_ptconecoreTrackPtrCorrection', 'p_r33over37allcalo', 'p_topoetconeCorrBitset', 'p_topoetconecoreConeEnergyCorrection', 'p_topoetconecoreConeSCEnergyCorrection', 'p_weta1', 'p_widths1', 'p_widths2', 'p_wtots1', 'p_e233', 'p_e237', 'p_e277', 'p_e2tsts1', 'p_ehad1', 'p_emaxs1', 'p_fracs1', 'p_DeltaE', 'p_E3x5_Lr0', 'p_E3x5_Lr1', 'p_E3x5_Lr2', 'p_E3x5_Lr3', 'p_E5x7_Lr0', 'p_E5x7_Lr1', 'p_E5x7_Lr2', 'p_E5x7_Lr3', 'p_E7x11_Lr0', 'p_E7x11_Lr1', 'p_E7x11_Lr2', 'p_E7x11_Lr3', 'p_E7x7_Lr0', 'p_E7x7_Lr1' ]\n",
    "\n",
    "all = train[all_variables]\n",
    "\n",
    "electron_truth = train['Truth']\n",
    "y = train['p_truth_E']\n",
    "y = y[electron_truth==1]\n",
    "X = train[all_variables]\n",
    "X = X[electron_truth==1]\n",
    "y = y.to_numpy(dtype='float32')\n",
    "\n",
    "# print(y)\n",
    "# prepocessing\n",
    "\n",
    "norm = preprocessing.MinMaxScaler()\n",
    "sc_X = preprocessing.StandardScaler()\n",
    "sc_y = preprocessing.StandardScaler()\n",
    "X = sc_X.fit_transform(X)\n",
    "y = np.reshape(y, (-1,1))\n",
    "y = sc_y.fit_transform(y)\n",
    "y = y.ravel()\n",
    "\n",
    "print(y)\n",
    "\n",
    "X = pd.DataFrame(X, columns=all_variables)\n",
    "# y = pd.DataFrame(y, columns=['p_truth_E'])\n",
    "\n",
    "\n",
    "print (f'Shape of X: {X.shape}')\n",
    "print (f'Shape of y: {y.shape}')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_selection\n",
    "def selection_features(X_train, y_train, X_test):\n",
    "    select = SelectKBest(score_func=mutual_info_regression, k=12)\n",
    "    select.fit(X_train, y_train)\n",
    "    \n",
    "    return select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select = selection_features(X_train, y_train, X_test)\n",
    "# X = pd.DataFrame(X)\n",
    "# names = X.columns.values[select.get_support()]\n",
    "# scores = select.scores_[select.get_support()]\n",
    "# names_scores = list(zip(names, scores))\n",
    "# ns_df = pd.DataFrame(data = names_scores, columns=['Feature_names', 'Feature_scores'])\n",
    "# #Sort the dataframe for better visualization\n",
    "# ns_df_sorted = ns_df.sort_values(['Feature_names', 'Feature_scores'], ascending = [False, True])\n",
    "# print(ns_df_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_feature_importance():\n",
    "#     model = SelectKBest(mutual_info_regression, k=12)#选择k个最佳特征\n",
    "#     X_new = model.fit_transform(X, y)\n",
    "#     #feature_data是特征数据，label_data是标签数据，该函数可以选择出k个特征 \n",
    " \n",
    "#     print(\"model shape: \",X_new.shape)\n",
    " \n",
    "#     scores = model.scores_\n",
    "#     print('model scores:', scores)  # 得分越高，特征越重要\n",
    " \n",
    "#     p_values = model.pvalues_\n",
    "#     print('model p-values', p_values)  # p-values 越小，置信度越高，特征越重要\n",
    " \n",
    "#     # 按重要性排序，选出最重要的 k 个\n",
    "#     indices = np.argsort(scores)[::-1]\n",
    "#     k_best_features = list(X.columns.values[indices[0:12]])\n",
    " \n",
    "#     print('k best features are: ',k_best_features)\n",
    "    \n",
    "#     return k_best_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_variables = ['p_eCluster',\n",
    " 'p_rawECluster',\n",
    " 'p_eAccCluster',\n",
    " 'p_ecore',\n",
    " 'p_E7x11_Lr2',\n",
    " 'p_E7x7_Lr2',\n",
    " 'p_e277',\n",
    " 'p_eClusterLr2',\n",
    " 'p_E5x7_Lr2',\n",
    " 'p_e255',\n",
    " 'p_e237',\n",
    " 'p_e235']\n",
    "\n",
    "\n",
    "\n",
    "X_lgb = train[lgb_variables]\n",
    "X_lgb = X_lgb[electron_truth==1]\n",
    "X_lgb = sc_X.fit_transform(X_lgb)\n",
    "X_lgb = pd.DataFrame(X_lgb, columns=lgb_variables)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "# predict\n",
    "\n",
    "# optimize hyperparameters of lgb\n",
    "\n",
    "# def objective_lgb(trial):\n",
    "\n",
    "#     X_lgb_train, X_lgb_test, y_lgb_train, y_lgb_test = train_test_split(X_lgb, y, test_size=0.2, random_state=42)\n",
    "#     train_data = lgb.Dataset(X_lgb_train, label=y_lgb_train)\n",
    "#     valid_data = lgb.Dataset(X_lgb_test, label=y_lgb_test)\n",
    "    \n",
    "#     boosting_types = [\"gbdt\", \"rf\", \"dart\"]\n",
    "#     boosting_type = trial.suggest_categorical(\"boosting_type\", boosting_types)\n",
    "\n",
    "#     params = {\n",
    "#         'objective': 'regression',\n",
    "#         'boosting_type': trial.suggest_categorical(\"boosting_type\", [\"gbdt\", \"rf\", \"dart\"]),\n",
    "#         'max_depth': trial.suggest_int('max_depth', 2, 100),\n",
    "#         'min_child_samples': trial.suggest_int('min_child_samples', 0, 1000),\n",
    "#         'metric': 'mae',\n",
    "#         'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.5),\n",
    "#         'feature_fraction': trial.suggest_float('feature_fraction', 0.2, 0.95, step=0.1),\n",
    "#         'num_leaves': trial.suggest_int('num_leaves', 20, 3000, step=20),\n",
    "#         'bagging_freq': trial.suggest_categorical('bagging_freq', [1]),\n",
    "#         'bagging_fraction': trial.suggest_float('bagging_fraction', 0.2, 0.95, step=0.1),\n",
    "#         'reg_alpha': trial.suggest_float(\"reg_alpha\", 0, 100, step=0.1),\n",
    "#         'reg_lambda': trial.suggest_int(\"reg_lambda\", 0, 1000, step=1),\n",
    "#         'verbosity': -1,\n",
    "#     }\n",
    "\n",
    "#     lgbm = LGBMRegressor(**params)\n",
    "#     lgbm.fit(X_lgb_train, y_lgb_train, eval_set=[(X_lgb_test, y_lgb_test)],early_stopping_rounds=100, verbose=False)\n",
    "#     pred_lgb=lgbm.predict(X_lgb_test)\n",
    "#     mae = mean_absolute_error(y_lgb_test, pred_lgb)\n",
    "#     return mae\n",
    "\n",
    "\n",
    "# study = optuna.create_study(\n",
    "#     direction=\"maximize\",\n",
    "#     sampler=TPESampler(seed=42),\n",
    "#     pruner=MedianPruner(n_warmup_steps=50),\n",
    "# ) \n",
    "\n",
    "# study.optimize(objective_lgb, n_trials=100, show_progress_bar=True)\n",
    "\n",
    "# study.best_trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimized_lgb():\n",
    "    X_lgb_train, X_lgb_test, y_lgb_train, y_lgb_test = train_test_split(X_lgb, y, test_size=0.2, random_state=42)\n",
    "    train_data = lgb.Dataset(X_lgb_train, label=y_lgb_train)\n",
    "    valid_data = lgb.Dataset(X_lgb_test, label=y_lgb_test)\n",
    "\n",
    "    # params = {\n",
    "    #     'objective': 'regression',\n",
    "    #     'metric': 'mae',\n",
    "    #     'boosting_type': 'rf',\n",
    "    #     'max_depth': 2,\n",
    "    #     'min_child_samples': 74,\n",
    "    #     'learning_rate': 0.4319464280296327,\n",
    "    #     'feature_fraction': 0.9,\n",
    "    #     'num_leaves': 680,\n",
    "    #     'bagging_freq': 1,\n",
    "    #     'bagging_fraction': 0.2,\n",
    "    #     'reg_alpha': 63.5,\n",
    "    #     'reg_lambda': 985,\n",
    "    #     'verbosity': -1,\n",
    "    # }\n",
    "\n",
    "    params = {\n",
    "        'objective': 'regression',\n",
    "        'metric': 'mae',\n",
    "        'boosting_type': 'rf',\n",
    " 'max_depth': 2,\n",
    " 'min_child_samples': 323,\n",
    " 'learning_rate': 0.38183517656611227,\n",
    " 'feature_fraction': 0.8,\n",
    " 'num_leaves': 2580,\n",
    " 'bagging_freq': 1,\n",
    " 'bagging_fraction': 0.2,\n",
    " 'reg_alpha': 88.0,\n",
    " 'reg_lambda': 835,\n",
    "        'verbosity': -1,\n",
    "    }\n",
    "\n",
    "    lgbm = LGBMRegressor(**params, num_boost_round=1000)\n",
    "    lgbm.fit(X_lgb_train, y_lgb_train, eval_set=[(X_lgb_test, y_lgb_test)],early_stopping_rounds=100, verbose=False)\n",
    "    pred_lgb=lgbm.predict(X_lgb_test)\n",
    "    mae = mean_absolute_error(y_lgb_test, pred_lgb)\n",
    "    r2s = r2_score(y_lgb_test, pred_lgb)\n",
    "    \n",
    "    y_pred = lgbm.predict(X_lgb_test.values)\n",
    "    y_pred = pd.DataFrame(data=y_pred)\n",
    "    y_pred = sc_y.inverse_transform(y_pred)\n",
    "    acc = accuracy_score(y_pred, y)\n",
    "    \n",
    "    return mae, r2s, y_pred\n",
    "\n",
    "optimized_lgb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural network hyper\n",
    "\n",
    "params = {'max_depth': poisson(25), \n",
    "          'min_samples_leaf': randint(1, 100)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- TRAINING ---------\n",
      "Epoch 1/5\n",
      "3038/3038 [==============================] - 2s 661us/step - loss: 0.1629 - mae: 0.1629 - val_loss: 0.1574 - val_mae: 0.1574\n",
      "Epoch 2/5\n",
      "3038/3038 [==============================] - 2s 631us/step - loss: 0.1460 - mae: 0.1460 - val_loss: 0.1448 - val_mae: 0.1448\n",
      "Epoch 3/5\n",
      "3038/3038 [==============================] - 2s 634us/step - loss: 0.1439 - mae: 0.1439 - val_loss: 0.1461 - val_mae: 0.1461\n",
      "Epoch 4/5\n",
      "3038/3038 [==============================] - 2s 640us/step - loss: 0.1425 - mae: 0.1425 - val_loss: 0.1354 - val_mae: 0.1354\n",
      "Epoch 5/5\n",
      "3038/3038 [==============================] - 2s 636us/step - loss: 0.1416 - mae: 0.1416 - val_loss: 0.1363 - val_mae: 0.1363\n",
      "760/760 - 0s - loss: 0.1363 - mae: 0.1363 - 307ms/epoch - 404us/step\n",
      "760/760 [==============================] - 0s 393us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.13634458184242249, 0.13634458184242249],\n",
       " 0.13634458,\n",
       " array([[-0.48221755],\n",
       "        [ 0.7928586 ],\n",
       "        [-0.34198034],\n",
       "        ...,\n",
       "        [ 0.3784817 ],\n",
       "        [-0.2381333 ],\n",
       "        [ 0.30047542]], dtype=float32),\n",
       " 0.9004806429738038)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# neural network\n",
    "\n",
    "def optimized_nn():\n",
    "    X_lgb_train, X_lgb_test, y_lgb_train, y_lgb_test = train_test_split(X_lgb, y, test_size=0.2, random_state=12)\n",
    "    model = Sequential([\n",
    "        Dense(9,activation='relu',name='input_layer'),\n",
    "        Dense(24,activation='relu',name='hidden_layer1'),\n",
    "        Dense(12,activation='relu',name='hidden_layer2'),\n",
    "        Dense(1, name='output')])\n",
    "\n",
    "    optimizer = tf.keras.optimizers.RMSprop(0.001)\n",
    "    model.compile(optimizer=optimizer,\n",
    "                loss='mae',\n",
    "                metrics='mae')\n",
    "\n",
    "    print('--------- TRAINING ---------')\n",
    "    history = model.fit(x=X_lgb_train, y=y_lgb_train, validation_data=(X_lgb_test, y_lgb_test), epochs = 5)  \n",
    "    score = model.evaluate(X_lgb_test,  y_lgb_test, verbose=2)\n",
    "    y_pred = model.predict(X_lgb_test)\n",
    "    mae = mean_absolute_error(y_pred, y_lgb_test)\n",
    "    # y_pred = sc_y.inverse_transform(y_pred)\n",
    "\n",
    "    return score, mae, y_pred, r2_score(y_pred, y_lgb_test)\n",
    "\n",
    "optimized_nn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgboost\n",
    "\n",
    "def shap_xgbooster():\n",
    "    model = xgboost.XGBRegressor().fit(X, y)\n",
    "    explainer = shap.Explainer(model)\n",
    "    shap_values = explainer(X)\n",
    "\n",
    "    feature_names = shap_values.feature_names\n",
    "    shap_df = pd.DataFrame(shap_values.values, columns=feature_names)\n",
    "    vals = np.abs(shap_df.values).mean(0)\n",
    "    shap_importance = pd.DataFrame(list(zip(feature_names, vals)), columns=['col_name', 'feature_importance_vals'])\n",
    "    shap_importance.sort_values(by=['feature_importance_vals'], ascending=False, inplace=True)\n",
    "    shap.plots.bar(shap_values)\n",
    "\n",
    "    return shap_importance.head(20)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4e3119b6b440005e83014445b502bc062a01c9850f9c4ea1b0d68db6d948f423"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
