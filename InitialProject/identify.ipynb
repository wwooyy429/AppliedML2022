{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import pandas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import tree\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "import shap\n",
    "import lightgbm as lgb \n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from optuna.integration import LightGBMPruningCallback\n",
    "from optuna.pruners import MedianPruner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "def load_data(name):\n",
    "    with h5py.File(f'{name}.h5', 'r') as f:\n",
    "        filename = name.split('/')[-1]\n",
    "        return pandas.DataFrame(f[filename][:], dtype=np.float64)\n",
    "\n",
    "train = load_data('train')\n",
    "test  = load_data('test')\n",
    "\n",
    "print (f'Shape of training data set: {train.shape}')\n",
    "print (f'Shape of test data set: {test.shape}')\n",
    "\n",
    "all_variables = ['actualInteractionsPerCrossing', 'averageInteractionsPerCrossing', 'correctedActualMu', 'correctedAverageMu', 'correctedScaledActualMu', 'correctedScaledAverageMu', 'NvtxReco', 'p_nTracks', 'p_pt_track', 'p_eta', 'p_phi', 'p_charge', 'p_qOverP', 'p_z0', 'p_d0', 'p_sigmad0', 'p_d0Sig', 'p_EptRatio', 'p_dPOverP', 'p_z0theta', 'p_etaCluster', 'p_phiCluster', 'p_eCluster', 'p_rawEtaCluster', 'p_rawPhiCluster', 'p_rawECluster', 'p_eClusterLr0', 'p_eClusterLr1', 'p_eClusterLr2', 'p_eClusterLr3', 'p_etaClusterLr1', 'p_etaClusterLr2', 'p_phiClusterLr2', 'p_eAccCluster', 'p_f0Cluster', 'p_etaCalo', 'p_phiCalo', 'p_eTileGap3Cluster', 'p_cellIndexCluster', 'p_phiModCalo', 'p_etaModCalo', 'p_dPhiTH3', 'p_R12', 'p_fTG3', 'p_weta2', 'p_Reta', 'p_Rphi', 'p_Eratio', 'p_f1', 'p_f3', 'p_Rhad', 'p_Rhad1', 'p_deltaEta1', 'p_deltaPhiRescaled2', 'p_TRTPID', 'p_TRTTrackOccupancy', 'p_numberOfInnermostPixelHits', 'p_numberOfPixelHits', 'p_numberOfSCTHits', 'p_numberOfTRTHits', 'p_numberOfTRTXenonHits', 'p_chi2', 'p_ndof', 'p_SharedMuonTrack', 'p_E7x7_Lr2', 'p_E7x7_Lr3', 'p_E_Lr0_HiG', 'p_E_Lr0_LowG', 'p_E_Lr0_MedG', 'p_E_Lr1_HiG', 'p_E_Lr1_LowG', 'p_E_Lr1_MedG', 'p_E_Lr2_HiG', 'p_E_Lr2_LowG', 'p_E_Lr2_MedG', 'p_E_Lr3_HiG', 'p_E_Lr3_LowG', 'p_E_Lr3_MedG', 'p_ambiguityType', 'p_asy1', 'p_author', 'p_barys1', 'p_core57cellsEnergyCorrection', 'p_deltaEta0', 'p_deltaEta2', 'p_deltaEta3', 'p_deltaPhi0', 'p_deltaPhi1', 'p_deltaPhi2', 'p_deltaPhi3', 'p_deltaPhiFromLastMeasurement', 'p_deltaPhiRescaled0', 'p_deltaPhiRescaled1', 'p_deltaPhiRescaled3', 'p_e1152', 'p_e132', 'p_e235', 'p_e255', 'p_e2ts1', 'p_ecore', 'p_emins1', 'p_etconeCorrBitset', 'p_ethad', 'p_ethad1', 'p_f1core', 'p_f3core', 'p_maxEcell_energy', 'p_maxEcell_gain', 'p_maxEcell_time', 'p_maxEcell_x', 'p_maxEcell_y', 'p_maxEcell_z', 'p_nCells_Lr0_HiG', 'p_nCells_Lr0_LowG', 'p_nCells_Lr0_MedG', 'p_nCells_Lr1_HiG', 'p_nCells_Lr1_LowG', 'p_nCells_Lr1_MedG', 'p_nCells_Lr2_HiG', 'p_nCells_Lr2_LowG', 'p_nCells_Lr2_MedG', 'p_nCells_Lr3_HiG', 'p_nCells_Lr3_LowG', 'p_nCells_Lr3_MedG', 'p_pos', 'p_pos7', 'p_poscs1', 'p_poscs2', 'p_ptconeCorrBitset', 'p_ptconecoreTrackPtrCorrection', 'p_r33over37allcalo', 'p_topoetconeCorrBitset', 'p_topoetconecoreConeEnergyCorrection', 'p_topoetconecoreConeSCEnergyCorrection', 'p_weta1', 'p_widths1', 'p_widths2', 'p_wtots1', 'p_e233', 'p_e237', 'p_e277', 'p_e2tsts1', 'p_ehad1', 'p_emaxs1', 'p_fracs1', 'p_DeltaE', 'p_E3x5_Lr0', 'p_E3x5_Lr1', 'p_E3x5_Lr2', 'p_E3x5_Lr3', 'p_E5x7_Lr0', 'p_E5x7_Lr1', 'p_E5x7_Lr2', 'p_E5x7_Lr3', 'p_E7x11_Lr0', 'p_E7x11_Lr1', 'p_E7x11_Lr2', 'p_E7x11_Lr3', 'p_E7x7_Lr0', 'p_E7x7_Lr1' ]\n",
    "\n",
    "X = train[all_variables]\n",
    "y = train['Truth']\n",
    "\n",
    "print (f'Shape of X: {X.shape}')\n",
    "print (f'Shape of y: {y.shape}')\n",
    "\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decision tree\n",
    "def decision_tree(X_train, X_test, y_train, y_test):\n",
    "\n",
    "\n",
    "    clf = DecisionTreeClassifier()\n",
    "\n",
    "    model = clf.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    return y_pred, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shap_lgb():\n",
    "    \n",
    "\n",
    "explainer = shap.Explainer(decision_tree.model.predict, X_test)\n",
    "# explainer = shap.Explainer(decision_tree.model)\n",
    "# shap_values = explainer(X_train)\n",
    "shap_values = explainer(X_test)\n",
    "# print(shap_values)\n",
    "shap.plots.bar(shap_values)\n",
    "\n",
    "feature_names = shap_values.feature_names\n",
    "shap_df = pd.DataFrame(shap_values.values, columns=feature_names)\n",
    "vals = np.abs(shap_df.values).mean(0)\n",
    "shap_importance = pd.DataFrame(list(zip(feature_names, vals)), columns=['col_name', 'feature_importance_vals'])\n",
    "shap_importance.sort_values(by=['feature_importance_vals'], ascending=False, inplace=True)\n",
    "\n",
    "shap_importance.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_variables = ['p_Rhad', 'p_Rphi', 'p_Reta', 'p_sigmad0', 'p_deltaEta1', 'p_ptconecoreTrackPtrCorrection', 'p_deltaPhiRescaled2', 'p_d0', 'p_numberOfInnermostPixelHits', 'p_ambiguityType',\n",
    "                    'p_rawPhiCluster','p_phiCalo', 'p_ethad', 'p_EptRatio', 'p_Rhad1', 'p_E7x11_Lr3', 'p_ehad1', 'p_Eratio', 'p_deltaPhi2', 'p_nTracks']\n",
    "\n",
    "# split data with shap\n",
    "X = train[shape_variables]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(np.shape(decision_tree(X_train, X_test, y_train, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimize hyperparameters of lgb\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "valid_data = lgb.Dataset(X_test, label=y_test)\n",
    "\n",
    "def objective_lgb(trial):\n",
    "    \n",
    "    boosting_types = [\"gbdt\", \"rf\", \"dart\"]\n",
    "    boosting_type = trial.suggest_categorical(\"boosting_type\", boosting_types)\n",
    "\n",
    "    params = {\n",
    "        'objective': 'regression',\n",
    "        'boosting_type': trial.suggest_categorical(\"boosting_type\", [\"gbdt\", \"rf\", \"dart\"]),\n",
    "        'max_depth': trial.suggest_int('max_depth', 2, 12),\n",
    "        'metric': {'l2', 'auc'},\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.5),\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.2, 0.95, step=0.1),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 3000, step=20),\n",
    "        'bagging_freq': trial.suggest_categorical('bagging_freq', [1]),\n",
    "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.2, 0.95, step=0.1),\n",
    "        'reg_alpha': trial.suggest_float(\"reg_alpha\", 0, 100, step=0.1),\n",
    "        'reg_lambda': trial.suggest_int(\"reg_lambda\", 0, 1000, step=1),\n",
    "        'verbosity': -1,\n",
    "    }\n",
    "\n",
    "    N_iterations_max = 10000\n",
    "    early_stopping_rounds = 50\n",
    "\n",
    "    if boosting_type == \"dart\":\n",
    "        N_iterations_max = 100\n",
    "        early_stopping_rounds = None\n",
    "\n",
    "    cv_res = lgb.cv(\n",
    "        params,\n",
    "        train_data,\n",
    "        num_boost_round=N_iterations_max,\n",
    "        early_stopping_rounds=early_stopping_rounds,\n",
    "        verbose_eval=False,\n",
    "        seed=42,\n",
    "        callbacks=[LightGBMPruningCallback(trial, \"auc\")],\n",
    "    )\n",
    "\n",
    "    num_boost_round = len(cv_res[\"auc-mean\"])\n",
    "    trial.set_user_attr(\"num_boost_round\", num_boost_round)\n",
    "\n",
    "    return cv_res[\"auc-mean\"][-1]\n",
    "\n",
    "\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction=\"maximize\",\n",
    "    sampler=TPESampler(seed=42),\n",
    "    pruner=MedianPruner(n_warmup_steps=50),\n",
    ")\n",
    "\n",
    "study.optimize(objective_lgb, n_trials=100, show_progress_bar=True)\n",
    "\n",
    "study.best_trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import roc_curve\n",
    "# from sklearn.metrics import auc\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# fpr, tpr, _ = roc_curve(y_test1, y_score) \n",
    "# auc_score = auc(fpr,tpr)   \n",
    "# auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimized lgb\n",
    "\n",
    "def optimized_lgb(X_train, X_test, y_train, y_test):\n",
    "    train_data = lgb.Dataset(X_train, label=y_train)\n",
    "    test_data = lgb.Dataset(X_test, label=y_test)\n",
    "    params = {'objective': 'regression',\n",
    "            'metric': 'binary_logloss',\n",
    "            'boosting_type': 'dart',\n",
    "            'max_depth': 10,\n",
    "            'learning_rate': 0.2759844445088989,\n",
    "            'feature_fraction': 0.8,\n",
    "            'num_leaves': 360,\n",
    "            'bagging_freq': 1,\n",
    "            'bagging_fraction': 0.8,\n",
    "            'reg_alpha': 8.200000000000001,\n",
    "            'reg_lambda': 103,\n",
    "            'verbose':-1,\n",
    "            'force_col_wise': True}\n",
    "\n",
    "    lgb_clf = lgb.train(params, train_set=train_data, num_boost_round=1000)\n",
    "    y_pred = np.around(lgb_clf.predict(X.values))\n",
    "    acc = accuracy_score(y_pred, y)\n",
    "    print(f\"Train accuracy: {acc*100.0:.2f}%\")\n",
    "\n",
    "    return lgb_clf.predict(X.values), acc\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'boosting_type': 'dart',\n",
    " 'max_depth': 10,\n",
    " 'learning_rate': 0.2759844445088989,\n",
    " 'feature_fraction': 0.8,\n",
    " 'num_leaves': 360,\n",
    " 'bagging_freq': 1,\n",
    " 'bagging_fraction': 0.8,\n",
    " 'reg_alpha': 8.200000000000001,\n",
    " 'reg_lambda': 103}"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4e3119b6b440005e83014445b502bc062a01c9850f9c4ea1b0d68db6d948f423"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
